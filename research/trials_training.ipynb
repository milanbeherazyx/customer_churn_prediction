{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Importing Libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Import\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "# Filter out a specific warning category\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Setting up the Project Working Directory`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/milan/customer_churn_prediction/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/milan/customer_churn_prediction'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  `Reading xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Location</th>\n",
       "      <th>Subscription_Length_Months</th>\n",
       "      <th>Monthly_Bill</th>\n",
       "      <th>Total_Usage_GB</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Customer_1</td>\n",
       "      <td>63</td>\n",
       "      <td>Male</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>17</td>\n",
       "      <td>73.36</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Customer_2</td>\n",
       "      <td>62</td>\n",
       "      <td>Female</td>\n",
       "      <td>New York</td>\n",
       "      <td>1</td>\n",
       "      <td>48.76</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Customer_3</td>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>5</td>\n",
       "      <td>85.47</td>\n",
       "      <td>460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Customer_4</td>\n",
       "      <td>36</td>\n",
       "      <td>Female</td>\n",
       "      <td>Miami</td>\n",
       "      <td>3</td>\n",
       "      <td>97.94</td>\n",
       "      <td>297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Customer_5</td>\n",
       "      <td>46</td>\n",
       "      <td>Female</td>\n",
       "      <td>Miami</td>\n",
       "      <td>19</td>\n",
       "      <td>58.14</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Customer_6</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>New York</td>\n",
       "      <td>15</td>\n",
       "      <td>82.65</td>\n",
       "      <td>456</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Customer_7</td>\n",
       "      <td>30</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>3</td>\n",
       "      <td>73.79</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Customer_8</td>\n",
       "      <td>67</td>\n",
       "      <td>Female</td>\n",
       "      <td>Miami</td>\n",
       "      <td>1</td>\n",
       "      <td>97.70</td>\n",
       "      <td>396</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Customer_9</td>\n",
       "      <td>20</td>\n",
       "      <td>Female</td>\n",
       "      <td>Miami</td>\n",
       "      <td>10</td>\n",
       "      <td>42.45</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Customer_10</td>\n",
       "      <td>53</td>\n",
       "      <td>Female</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>12</td>\n",
       "      <td>64.49</td>\n",
       "      <td>383</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Customer_11</td>\n",
       "      <td>50</td>\n",
       "      <td>Female</td>\n",
       "      <td>Miami</td>\n",
       "      <td>5</td>\n",
       "      <td>88.49</td>\n",
       "      <td>442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Customer_12</td>\n",
       "      <td>44</td>\n",
       "      <td>Male</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>20</td>\n",
       "      <td>95.71</td>\n",
       "      <td>295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Customer_13</td>\n",
       "      <td>34</td>\n",
       "      <td>Female</td>\n",
       "      <td>Miami</td>\n",
       "      <td>10</td>\n",
       "      <td>90.42</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Customer_14</td>\n",
       "      <td>28</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>13</td>\n",
       "      <td>58.48</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Customer_15</td>\n",
       "      <td>42</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>15</td>\n",
       "      <td>46.87</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Customer_16</td>\n",
       "      <td>55</td>\n",
       "      <td>Female</td>\n",
       "      <td>Houston</td>\n",
       "      <td>13</td>\n",
       "      <td>61.11</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Customer_17</td>\n",
       "      <td>43</td>\n",
       "      <td>Female</td>\n",
       "      <td>New York</td>\n",
       "      <td>8</td>\n",
       "      <td>81.40</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Customer_18</td>\n",
       "      <td>63</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>1</td>\n",
       "      <td>83.31</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Customer_19</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Miami</td>\n",
       "      <td>23</td>\n",
       "      <td>63.40</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Customer_20</td>\n",
       "      <td>41</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>1</td>\n",
       "      <td>58.76</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CustomerID         Name  Age  Gender     Location  \\\n",
       "0            1   Customer_1   63    Male  Los Angeles   \n",
       "1            2   Customer_2   62  Female     New York   \n",
       "2            3   Customer_3   24  Female  Los Angeles   \n",
       "3            4   Customer_4   36  Female        Miami   \n",
       "4            5   Customer_5   46  Female        Miami   \n",
       "5            6   Customer_6   67    Male     New York   \n",
       "6            7   Customer_7   30  Female      Chicago   \n",
       "7            8   Customer_8   67  Female        Miami   \n",
       "8            9   Customer_9   20  Female        Miami   \n",
       "9           10  Customer_10   53  Female  Los Angeles   \n",
       "10          11  Customer_11   50  Female        Miami   \n",
       "11          12  Customer_12   44    Male  Los Angeles   \n",
       "12          13  Customer_13   34  Female        Miami   \n",
       "13          14  Customer_14   28    Male      Chicago   \n",
       "14          15  Customer_15   42    Male      Chicago   \n",
       "15          16  Customer_16   55  Female      Houston   \n",
       "16          17  Customer_17   43  Female     New York   \n",
       "17          18  Customer_18   63    Male      Chicago   \n",
       "18          19  Customer_19   31    Male        Miami   \n",
       "19          20  Customer_20   41  Female      Chicago   \n",
       "\n",
       "    Subscription_Length_Months  Monthly_Bill  Total_Usage_GB  Churn  \n",
       "0                           17         73.36             236      0  \n",
       "1                            1         48.76             172      0  \n",
       "2                            5         85.47             460      0  \n",
       "3                            3         97.94             297      1  \n",
       "4                           19         58.14             266      0  \n",
       "5                           15         82.65             456      1  \n",
       "6                            3         73.79             269      0  \n",
       "7                            1         97.70             396      1  \n",
       "8                           10         42.45             150      1  \n",
       "9                           12         64.49             383      1  \n",
       "10                           5         88.49             442      0  \n",
       "11                          20         95.71             295      1  \n",
       "12                          10         90.42             148      1  \n",
       "13                          13         58.48             239      1  \n",
       "14                          15         46.87             233      0  \n",
       "15                          13         61.11             231      1  \n",
       "16                           8         81.40             231      1  \n",
       "17                           1         83.31             198      0  \n",
       "18                          23         63.40             236      0  \n",
       "19                           1         58.76             354      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the Excel file from the URL using pandas\n",
    "churn = pd.read_excel('./src/churn/data/customer_churn_large_dataset.xlsx')\n",
    "# View 1st 20 records\n",
    "churn.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Drop unnecessary columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns_to_drop):\n",
    "    \"\"\"\n",
    "    Drops specified columns from a DataFrame in place.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to modify.\n",
    "        columns_to_drop (list): List of column names to drop.\n",
    "    \"\"\"\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame named 'churn'\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['CustomerID', 'Name']\n",
    "\n",
    "# Call the function to drop the columns\n",
    "drop_columns(churn, columns_to_drop)\n",
    "\n",
    "# The 'churn' DataFrame is modified in place and now doesn't contain 'CustomerID' and 'Name' columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Split Dependent and Independent Variable as X and y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (100000, 6)\n",
      "y Shape: (100000,)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, target_column):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into features (X) and the target variable (y).\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): The input DataFrame.\n",
    "        target_column (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        X (DataFrame): The features.\n",
    "        y (Series): The target variable.\n",
    "    \"\"\"\n",
    "    # Extract the target variable (y)\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Extract the features (X) by dropping the target column\n",
    "    X = data.drop(columns=[target_column])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'churn' and 'Churn' with your actual DataFrame and target column name\n",
    "X, y = split_data(churn, 'Churn')\n",
    "\n",
    "# Check the shape and size of X and y\n",
    "print(\"X Shape:\", X.shape)\n",
    "print(\"y Shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "99995    1\n",
       "99996    0\n",
       "99997    1\n",
       "99998    1\n",
       "99999    1\n",
       "Name: Churn, Length: 100000, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_data_balanced(data, target_column, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Splits the data into training and test sets with balanced class distribution.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame.\n",
    "        target_column (str): The name of the target column.\n",
    "        test_size (float): The proportion of the dataset to include in the test split (default is 0.2).\n",
    "        random_state (int): Seed for the random number generator (default is None).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame, pd.Series, pd.Series: Returns X_train, X_test, y_train, and y_test.\n",
    "    \"\"\"\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Split the data into training and test sets with stratified sampling\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Now you can use the function to split your data\n",
    "X_train, X_test, y_train, y_test = split_data_balanced(churn, target_column=\"Churn\", test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_data(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Concatenates X_train, X_test, y_train, and y_test to create train_data and test_data DataFrames.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Features for training.\n",
    "        X_test (pd.DataFrame): Features for testing.\n",
    "        y_train (pd.Series): Target labels for training.\n",
    "        y_test (pd.Series): Target labels for testing.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame: Returns train_data and test_data DataFrames.\n",
    "    \"\"\"\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# Now you can use the function to create train_data and test_data\n",
    "train_data, test_data = create_train_test_data(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_balance(train_data, test_data, target_column):\n",
    "    \"\"\"\n",
    "    Checks if the class balance between the train and test data is similar.\n",
    "    \n",
    "    Args:\n",
    "        train_data (pd.DataFrame): The training data.\n",
    "        test_data (pd.DataFrame): The test data.\n",
    "        target_column (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if class balance is similar, False otherwise.\n",
    "    \"\"\"\n",
    "    train_class_counts = train_data[target_column].value_counts(normalize=True)\n",
    "    test_class_counts = test_data[target_column].value_counts(normalize=True)\n",
    "\n",
    "    # Tolerance for class distribution difference\n",
    "    tolerance = 0.05  # Adjust this threshold as needed\n",
    "\n",
    "    for class_label, train_ratio in train_class_counts.items():\n",
    "        test_ratio = test_class_counts.get(class_label, 0)\n",
    "        if abs(train_ratio - test_ratio) > tolerance:\n",
    "            return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution is similar between train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Check if the class balance is similar between train and test sets\n",
    "is_similar = check_class_balance(train_data, test_data, target_column=\"Churn\")\n",
    "\n",
    "if is_similar:\n",
    "    print(\"Class distribution is similar between train and test sets.\")\n",
    "else:\n",
    "    print(\"Class distribution is not similar between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['Churn'].value_counts().plot(kind='barh', figsize=(8, 6))\n",
    "plt.xlabel(\"Count\", labelpad=14)\n",
    "plt.ylabel(\"Target Variable\", labelpad=14)\n",
    "plt.title(\"Count of TARGET Variable per category\", y=1.02);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data['Churn'].value_counts().plot(kind='barh', figsize=(8, 6))\n",
    "plt.xlabel(\"Count\", labelpad=14)\n",
    "plt.ylabel(\"Target Variable\", labelpad=14)\n",
    "plt.title(\"Count of TARGET Variable per category\", y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Define categorical_cols and numerical_cols`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['Gender',\n",
    "                    'Location']\n",
    "numerical_cols = ['Age',\n",
    "                  'Subscription_Length_Months',\n",
    "                  'Monthly_Bill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Feature Statistics:\n",
      "                             Count        Mean     Std Dev   Min     25%  \\\n",
      "Age                         100000   44.027020   15.280283  18.0   31.00   \n",
      "Subscription_Length_Months  100000   12.490100    6.926461   1.0    6.00   \n",
      "Monthly_Bill                100000   65.053197   20.230696  30.0   47.54   \n",
      "Total_Usage_GB              100000  274.393650  130.463063  50.0  161.00   \n",
      "Churn                       100000    0.497790    0.499998   0.0    0.00   \n",
      "\n",
      "                               50%     75%    Max  Skewness  Outliers  \n",
      "Age                          44.00   57.00   70.0 -0.002689         0  \n",
      "Subscription_Length_Months   12.00   19.00   24.0 -0.001655         0  \n",
      "Monthly_Bill                 65.01   82.64  100.0 -0.000326         0  \n",
      "Total_Usage_GB              274.00  387.00  500.0  0.007113         0  \n",
      "Churn                         0.00    1.00    1.0  0.008840         0  \n",
      "Best Scaling Method: StandardScaler\n"
     ]
    }
   ],
   "source": [
    "def scaling_stats(df):\n",
    "    # Filter only numeric columns\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "    # Calculate basic statistics for numeric columns\n",
    "    numerical_stats = numeric_df.describe()\n",
    "\n",
    "    # Check for skewness in numeric columns\n",
    "    skewness = numeric_df.skew()\n",
    "\n",
    "    # Check for outliers using IQR\n",
    "    Q1 = numeric_df.quantile(0.25)\n",
    "    Q3 = numeric_df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers_count = ((numeric_df < lower_bound) | (numeric_df > upper_bound)).sum()\n",
    "\n",
    "    # Create a summary dataframe\n",
    "    summary = pd.DataFrame({\n",
    "        'Count': numeric_df.count(),\n",
    "        'Mean': numerical_stats.loc['mean'],\n",
    "        'Std Dev': numerical_stats.loc['std'],\n",
    "        'Min': numerical_stats.loc['min'],\n",
    "        '25%': numerical_stats.loc['25%'],\n",
    "        '50%': numerical_stats.loc['50%'],\n",
    "        '75%': numerical_stats.loc['75%'],\n",
    "        'Max': numerical_stats.loc['max'],\n",
    "        'Skewness': skewness,\n",
    "        'Outliers': outliers_count\n",
    "    })\n",
    "\n",
    "    # Determine the best scaling method based on skewness\n",
    "    best_scaling_method = 'StandardScaler' if skewness.mean() < 1 else 'RobustScaler'\n",
    "\n",
    "    return summary, best_scaling_method\n",
    "\n",
    "\n",
    "summary, best_scaling = scaling_stats(churn)\n",
    "print(\"Numerical Feature Statistics:\")\n",
    "print(summary)\n",
    "print(f\"Best Scaling Method: {best_scaling}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Feature Engineering Automation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Pipeline for the selected categorical columns\n",
    "cat_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Handling Missing values\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))   # One-hot encoding\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Numerical Pipeline for the selected numerical columns\n",
    "num_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),  # Missing Values\n",
    "        ('scaler', StandardScaler())  # Feature Scaling\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use ColumnTransformer to apply the pipelines to the respective columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numerical_cols),\n",
    "        ('cat', cat_pipeline, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing to X_train and X_test\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.37436712, -0.07091502, -0.84196212, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.84922233, -0.07091502, -0.43956732, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.6994183 ,  0.50749615, -0.05593449, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.98002171,  0.36289336,  0.0289881 , ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.43976681,  1.08590731, -1.05574979, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.72037023,  0.94130452,  0.93153741, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of customer churn prediction, I would suggest prioritizing **Recall** as the best evaluation metric. Here's why:\n",
    "\n",
    "**Recommended Metric: Recall**\n",
    "\n",
    "**Reasoning:**\n",
    "- **Churn Identification**: The primary goal in customer churn prediction is to identify as many customers who are actually churning as possible. High recall ensures that the model captures a significant portion of churned customers, reducing the risk of losing revenue due to missed churn cases.\n",
    "\n",
    "- **Customer Retention**: Recall helps in minimizing false negatives, which are cases where the model fails to identify customers who are churning. By maximizing recall, you can focus on retaining customers who are truly at risk, leading to better customer retention efforts.\n",
    "\n",
    "- **Cost of Missed Churn**: Missing churned customers can be costly for an organization, as it may result in lost revenue. High recall helps in reducing this cost by ensuring that most churn cases are correctly identified.\n",
    "\n",
    "While Recall is the primary focus, it's important to keep in mind that precision is also valuable. Therefore, you should aim for a balanced approach by considering the F1-Score or other metrics that strike a balance between precision and recall.\n",
    "\n",
    "In your short report, you can mention that maximizing Recall is recommended to ensure that the model effectively identifies customers at risk of churning, thus enabling the organization to implement targeted retention strategies and reduce churn-related revenue losses. Additionally, you can mention the importance of a balanced evaluation approach that considers both precision and recall for a comprehensive assessment of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "Training XGB Classifier...\n",
      "Training SVC...\n",
      "Training Isolation Forest...\n",
      "Training MLP Classifier...\n",
      "Training Naive Bayes...\n",
      "Training Logistic Regression...\n",
      "Training K-Nearest Neighbours...\n",
      "Training Support Vector Machine...\n",
      "Training Decision Tree...\n",
      "Training Bagging Decision Tree...\n",
      "Training Boosted Decision Tree...\n",
      "Training Random Forest Ensemble...\n",
      "Training Voting Classification...\n",
      "Training XG Boost...\n",
      "Training Cat Boost...\n",
      "Learning rate set to 0.06692\n",
      "0:\tlearn: 0.6930878\ttotal: 66.6ms\tremaining: 1m 6s\n",
      "1:\tlearn: 0.6930507\ttotal: 71.7ms\tremaining: 35.8s\n",
      "2:\tlearn: 0.6930153\ttotal: 75.6ms\tremaining: 25.1s\n",
      "3:\tlearn: 0.6929754\ttotal: 79.7ms\tremaining: 19.8s\n",
      "4:\tlearn: 0.6929465\ttotal: 83.6ms\tremaining: 16.6s\n",
      "5:\tlearn: 0.6929208\ttotal: 87ms\tremaining: 14.4s\n",
      "6:\tlearn: 0.6929071\ttotal: 89.8ms\tremaining: 12.7s\n",
      "7:\tlearn: 0.6928625\ttotal: 93.5ms\tremaining: 11.6s\n",
      "8:\tlearn: 0.6928413\ttotal: 96.8ms\tremaining: 10.7s\n",
      "9:\tlearn: 0.6928181\ttotal: 100ms\tremaining: 9.91s\n",
      "10:\tlearn: 0.6927720\ttotal: 103ms\tremaining: 9.27s\n",
      "11:\tlearn: 0.6927358\ttotal: 107ms\tremaining: 8.79s\n",
      "12:\tlearn: 0.6927171\ttotal: 110ms\tremaining: 8.35s\n",
      "13:\tlearn: 0.6926942\ttotal: 113ms\tremaining: 7.97s\n",
      "14:\tlearn: 0.6926524\ttotal: 117ms\tremaining: 7.66s\n",
      "15:\tlearn: 0.6926187\ttotal: 120ms\tremaining: 7.37s\n",
      "16:\tlearn: 0.6925817\ttotal: 123ms\tremaining: 7.11s\n",
      "17:\tlearn: 0.6925403\ttotal: 127ms\tremaining: 6.92s\n",
      "18:\tlearn: 0.6925271\ttotal: 130ms\tremaining: 6.7s\n",
      "19:\tlearn: 0.6924851\ttotal: 133ms\tremaining: 6.52s\n",
      "20:\tlearn: 0.6924560\ttotal: 136ms\tremaining: 6.35s\n",
      "21:\tlearn: 0.6924302\ttotal: 140ms\tremaining: 6.21s\n",
      "22:\tlearn: 0.6924012\ttotal: 143ms\tremaining: 6.06s\n",
      "23:\tlearn: 0.6923555\ttotal: 147ms\tremaining: 5.96s\n",
      "24:\tlearn: 0.6923112\ttotal: 150ms\tremaining: 5.85s\n",
      "25:\tlearn: 0.6922674\ttotal: 154ms\tremaining: 5.78s\n",
      "26:\tlearn: 0.6922337\ttotal: 158ms\tremaining: 5.69s\n",
      "27:\tlearn: 0.6921945\ttotal: 162ms\tremaining: 5.61s\n",
      "28:\tlearn: 0.6921584\ttotal: 165ms\tremaining: 5.54s\n",
      "29:\tlearn: 0.6921242\ttotal: 169ms\tremaining: 5.48s\n",
      "30:\tlearn: 0.6921079\ttotal: 173ms\tremaining: 5.41s\n",
      "31:\tlearn: 0.6920661\ttotal: 177ms\tremaining: 5.35s\n",
      "32:\tlearn: 0.6920382\ttotal: 181ms\tremaining: 5.3s\n",
      "33:\tlearn: 0.6920374\ttotal: 184ms\tremaining: 5.22s\n",
      "34:\tlearn: 0.6920052\ttotal: 188ms\tremaining: 5.17s\n",
      "35:\tlearn: 0.6919823\ttotal: 191ms\tremaining: 5.12s\n",
      "36:\tlearn: 0.6919516\ttotal: 195ms\tremaining: 5.08s\n",
      "37:\tlearn: 0.6919236\ttotal: 199ms\tremaining: 5.05s\n",
      "38:\tlearn: 0.6919089\ttotal: 202ms\tremaining: 4.99s\n",
      "39:\tlearn: 0.6918737\ttotal: 207ms\tremaining: 4.97s\n",
      "40:\tlearn: 0.6918423\ttotal: 211ms\tremaining: 4.94s\n",
      "41:\tlearn: 0.6918056\ttotal: 216ms\tremaining: 4.93s\n",
      "42:\tlearn: 0.6917686\ttotal: 220ms\tremaining: 4.89s\n",
      "43:\tlearn: 0.6917387\ttotal: 224ms\tremaining: 4.87s\n",
      "44:\tlearn: 0.6916977\ttotal: 228ms\tremaining: 4.84s\n",
      "45:\tlearn: 0.6916933\ttotal: 232ms\tremaining: 4.81s\n",
      "46:\tlearn: 0.6916696\ttotal: 236ms\tremaining: 4.79s\n",
      "47:\tlearn: 0.6916305\ttotal: 244ms\tremaining: 4.84s\n",
      "48:\tlearn: 0.6916117\ttotal: 248ms\tremaining: 4.81s\n",
      "49:\tlearn: 0.6915864\ttotal: 252ms\tremaining: 4.78s\n",
      "50:\tlearn: 0.6915592\ttotal: 256ms\tremaining: 4.76s\n",
      "51:\tlearn: 0.6915267\ttotal: 260ms\tremaining: 4.74s\n",
      "52:\tlearn: 0.6915011\ttotal: 264ms\tremaining: 4.72s\n",
      "53:\tlearn: 0.6914719\ttotal: 268ms\tremaining: 4.69s\n",
      "54:\tlearn: 0.6914495\ttotal: 272ms\tremaining: 4.67s\n",
      "55:\tlearn: 0.6914339\ttotal: 276ms\tremaining: 4.64s\n",
      "56:\tlearn: 0.6914050\ttotal: 279ms\tremaining: 4.62s\n",
      "57:\tlearn: 0.6913808\ttotal: 283ms\tremaining: 4.59s\n",
      "58:\tlearn: 0.6913478\ttotal: 287ms\tremaining: 4.57s\n",
      "59:\tlearn: 0.6913342\ttotal: 290ms\tremaining: 4.55s\n",
      "60:\tlearn: 0.6913122\ttotal: 294ms\tremaining: 4.53s\n",
      "61:\tlearn: 0.6912738\ttotal: 297ms\tremaining: 4.5s\n",
      "62:\tlearn: 0.6912519\ttotal: 301ms\tremaining: 4.48s\n",
      "63:\tlearn: 0.6912288\ttotal: 304ms\tremaining: 4.45s\n",
      "64:\tlearn: 0.6911904\ttotal: 308ms\tremaining: 4.43s\n",
      "65:\tlearn: 0.6911713\ttotal: 312ms\tremaining: 4.41s\n",
      "66:\tlearn: 0.6911384\ttotal: 315ms\tremaining: 4.39s\n",
      "67:\tlearn: 0.6911067\ttotal: 320ms\tremaining: 4.38s\n",
      "68:\tlearn: 0.6910822\ttotal: 323ms\tremaining: 4.36s\n",
      "69:\tlearn: 0.6910475\ttotal: 327ms\tremaining: 4.34s\n",
      "70:\tlearn: 0.6910301\ttotal: 331ms\tremaining: 4.33s\n",
      "71:\tlearn: 0.6910083\ttotal: 334ms\tremaining: 4.31s\n",
      "72:\tlearn: 0.6909894\ttotal: 338ms\tremaining: 4.29s\n",
      "73:\tlearn: 0.6909654\ttotal: 342ms\tremaining: 4.28s\n",
      "74:\tlearn: 0.6909385\ttotal: 346ms\tremaining: 4.27s\n",
      "75:\tlearn: 0.6909213\ttotal: 350ms\tremaining: 4.25s\n",
      "76:\tlearn: 0.6909057\ttotal: 353ms\tremaining: 4.22s\n",
      "77:\tlearn: 0.6908842\ttotal: 356ms\tremaining: 4.21s\n",
      "78:\tlearn: 0.6908619\ttotal: 360ms\tremaining: 4.19s\n",
      "79:\tlearn: 0.6908465\ttotal: 363ms\tremaining: 4.18s\n",
      "80:\tlearn: 0.6908268\ttotal: 366ms\tremaining: 4.15s\n",
      "81:\tlearn: 0.6907959\ttotal: 370ms\tremaining: 4.14s\n",
      "82:\tlearn: 0.6907754\ttotal: 373ms\tremaining: 4.12s\n",
      "83:\tlearn: 0.6907479\ttotal: 377ms\tremaining: 4.11s\n",
      "84:\tlearn: 0.6907310\ttotal: 381ms\tremaining: 4.1s\n",
      "85:\tlearn: 0.6907139\ttotal: 384ms\tremaining: 4.08s\n",
      "86:\tlearn: 0.6906877\ttotal: 387ms\tremaining: 4.06s\n",
      "87:\tlearn: 0.6906684\ttotal: 392ms\tremaining: 4.07s\n",
      "88:\tlearn: 0.6906405\ttotal: 397ms\tremaining: 4.06s\n",
      "89:\tlearn: 0.6906112\ttotal: 401ms\tremaining: 4.05s\n",
      "90:\tlearn: 0.6905725\ttotal: 405ms\tremaining: 4.05s\n",
      "91:\tlearn: 0.6905477\ttotal: 409ms\tremaining: 4.03s\n",
      "92:\tlearn: 0.6905348\ttotal: 412ms\tremaining: 4.02s\n",
      "93:\tlearn: 0.6904963\ttotal: 416ms\tremaining: 4s\n",
      "94:\tlearn: 0.6904716\ttotal: 419ms\tremaining: 3.99s\n",
      "95:\tlearn: 0.6904715\ttotal: 422ms\tremaining: 3.97s\n",
      "96:\tlearn: 0.6904536\ttotal: 426ms\tremaining: 3.96s\n",
      "97:\tlearn: 0.6904414\ttotal: 429ms\tremaining: 3.95s\n",
      "98:\tlearn: 0.6904225\ttotal: 433ms\tremaining: 3.94s\n",
      "99:\tlearn: 0.6904115\ttotal: 437ms\tremaining: 3.93s\n",
      "100:\tlearn: 0.6903997\ttotal: 441ms\tremaining: 3.92s\n",
      "101:\tlearn: 0.6903634\ttotal: 445ms\tremaining: 3.92s\n",
      "102:\tlearn: 0.6903530\ttotal: 449ms\tremaining: 3.91s\n",
      "103:\tlearn: 0.6903309\ttotal: 452ms\tremaining: 3.89s\n",
      "104:\tlearn: 0.6903069\ttotal: 456ms\tremaining: 3.88s\n",
      "105:\tlearn: 0.6902734\ttotal: 459ms\tremaining: 3.87s\n",
      "106:\tlearn: 0.6902546\ttotal: 462ms\tremaining: 3.86s\n",
      "107:\tlearn: 0.6902168\ttotal: 466ms\tremaining: 3.85s\n",
      "108:\tlearn: 0.6901983\ttotal: 469ms\tremaining: 3.84s\n",
      "109:\tlearn: 0.6901788\ttotal: 473ms\tremaining: 3.83s\n",
      "110:\tlearn: 0.6901555\ttotal: 476ms\tremaining: 3.81s\n",
      "111:\tlearn: 0.6901183\ttotal: 480ms\tremaining: 3.81s\n",
      "112:\tlearn: 0.6900867\ttotal: 484ms\tremaining: 3.8s\n",
      "113:\tlearn: 0.6900651\ttotal: 487ms\tremaining: 3.79s\n",
      "114:\tlearn: 0.6900397\ttotal: 491ms\tremaining: 3.78s\n",
      "115:\tlearn: 0.6900128\ttotal: 495ms\tremaining: 3.77s\n",
      "116:\tlearn: 0.6899863\ttotal: 498ms\tremaining: 3.76s\n",
      "117:\tlearn: 0.6899694\ttotal: 501ms\tremaining: 3.75s\n",
      "118:\tlearn: 0.6899515\ttotal: 505ms\tremaining: 3.74s\n",
      "119:\tlearn: 0.6899353\ttotal: 508ms\tremaining: 3.73s\n",
      "120:\tlearn: 0.6898987\ttotal: 512ms\tremaining: 3.72s\n",
      "121:\tlearn: 0.6898804\ttotal: 515ms\tremaining: 3.71s\n",
      "122:\tlearn: 0.6898677\ttotal: 519ms\tremaining: 3.7s\n",
      "123:\tlearn: 0.6898329\ttotal: 522ms\tremaining: 3.69s\n",
      "124:\tlearn: 0.6898002\ttotal: 526ms\tremaining: 3.68s\n",
      "125:\tlearn: 0.6897688\ttotal: 530ms\tremaining: 3.67s\n",
      "126:\tlearn: 0.6897374\ttotal: 534ms\tremaining: 3.67s\n",
      "127:\tlearn: 0.6897215\ttotal: 537ms\tremaining: 3.66s\n",
      "128:\tlearn: 0.6897012\ttotal: 541ms\tremaining: 3.65s\n",
      "129:\tlearn: 0.6896807\ttotal: 544ms\tremaining: 3.64s\n",
      "130:\tlearn: 0.6896684\ttotal: 548ms\tremaining: 3.63s\n",
      "131:\tlearn: 0.6896570\ttotal: 551ms\tremaining: 3.62s\n",
      "132:\tlearn: 0.6896460\ttotal: 555ms\tremaining: 3.61s\n",
      "133:\tlearn: 0.6896249\ttotal: 558ms\tremaining: 3.61s\n",
      "134:\tlearn: 0.6895956\ttotal: 562ms\tremaining: 3.6s\n",
      "135:\tlearn: 0.6895802\ttotal: 565ms\tremaining: 3.59s\n",
      "136:\tlearn: 0.6895551\ttotal: 569ms\tremaining: 3.59s\n",
      "137:\tlearn: 0.6895398\ttotal: 573ms\tremaining: 3.58s\n",
      "138:\tlearn: 0.6895150\ttotal: 577ms\tremaining: 3.57s\n",
      "139:\tlearn: 0.6895030\ttotal: 580ms\tremaining: 3.56s\n",
      "140:\tlearn: 0.6894805\ttotal: 584ms\tremaining: 3.56s\n",
      "141:\tlearn: 0.6894639\ttotal: 588ms\tremaining: 3.55s\n",
      "142:\tlearn: 0.6894523\ttotal: 592ms\tremaining: 3.55s\n",
      "143:\tlearn: 0.6894310\ttotal: 596ms\tremaining: 3.54s\n",
      "144:\tlearn: 0.6894200\ttotal: 600ms\tremaining: 3.54s\n",
      "145:\tlearn: 0.6893945\ttotal: 603ms\tremaining: 3.53s\n",
      "146:\tlearn: 0.6893635\ttotal: 607ms\tremaining: 3.52s\n",
      "147:\tlearn: 0.6893280\ttotal: 611ms\tremaining: 3.52s\n",
      "148:\tlearn: 0.6892996\ttotal: 615ms\tremaining: 3.51s\n",
      "149:\tlearn: 0.6892650\ttotal: 619ms\tremaining: 3.5s\n",
      "150:\tlearn: 0.6892421\ttotal: 622ms\tremaining: 3.5s\n",
      "151:\tlearn: 0.6892225\ttotal: 626ms\tremaining: 3.5s\n",
      "152:\tlearn: 0.6892202\ttotal: 629ms\tremaining: 3.48s\n",
      "153:\tlearn: 0.6892012\ttotal: 633ms\tremaining: 3.48s\n",
      "154:\tlearn: 0.6891601\ttotal: 637ms\tremaining: 3.47s\n",
      "155:\tlearn: 0.6891330\ttotal: 641ms\tremaining: 3.47s\n",
      "156:\tlearn: 0.6891156\ttotal: 645ms\tremaining: 3.46s\n",
      "157:\tlearn: 0.6890845\ttotal: 649ms\tremaining: 3.46s\n",
      "158:\tlearn: 0.6890444\ttotal: 652ms\tremaining: 3.45s\n",
      "159:\tlearn: 0.6890205\ttotal: 656ms\tremaining: 3.44s\n",
      "160:\tlearn: 0.6890089\ttotal: 660ms\tremaining: 3.44s\n",
      "161:\tlearn: 0.6889823\ttotal: 664ms\tremaining: 3.43s\n",
      "162:\tlearn: 0.6889498\ttotal: 667ms\tremaining: 3.43s\n",
      "163:\tlearn: 0.6889341\ttotal: 671ms\tremaining: 3.42s\n",
      "164:\tlearn: 0.6889066\ttotal: 675ms\tremaining: 3.42s\n",
      "165:\tlearn: 0.6888921\ttotal: 678ms\tremaining: 3.41s\n",
      "166:\tlearn: 0.6888668\ttotal: 682ms\tremaining: 3.4s\n",
      "167:\tlearn: 0.6888347\ttotal: 687ms\tremaining: 3.4s\n",
      "168:\tlearn: 0.6888093\ttotal: 690ms\tremaining: 3.39s\n",
      "169:\tlearn: 0.6887814\ttotal: 695ms\tremaining: 3.39s\n",
      "170:\tlearn: 0.6887574\ttotal: 699ms\tremaining: 3.39s\n",
      "171:\tlearn: 0.6887401\ttotal: 703ms\tremaining: 3.38s\n",
      "172:\tlearn: 0.6887078\ttotal: 707ms\tremaining: 3.38s\n",
      "173:\tlearn: 0.6886763\ttotal: 711ms\tremaining: 3.37s\n",
      "174:\tlearn: 0.6886460\ttotal: 715ms\tremaining: 3.37s\n",
      "175:\tlearn: 0.6886194\ttotal: 718ms\tremaining: 3.36s\n",
      "176:\tlearn: 0.6885977\ttotal: 723ms\tremaining: 3.36s\n",
      "177:\tlearn: 0.6885708\ttotal: 727ms\tremaining: 3.36s\n",
      "178:\tlearn: 0.6885487\ttotal: 731ms\tremaining: 3.35s\n",
      "179:\tlearn: 0.6885199\ttotal: 736ms\tremaining: 3.35s\n",
      "180:\tlearn: 0.6884836\ttotal: 740ms\tremaining: 3.35s\n",
      "181:\tlearn: 0.6884536\ttotal: 744ms\tremaining: 3.35s\n",
      "182:\tlearn: 0.6884265\ttotal: 749ms\tremaining: 3.34s\n",
      "183:\tlearn: 0.6884100\ttotal: 752ms\tremaining: 3.33s\n",
      "184:\tlearn: 0.6883808\ttotal: 756ms\tremaining: 3.33s\n",
      "185:\tlearn: 0.6883491\ttotal: 760ms\tremaining: 3.33s\n",
      "186:\tlearn: 0.6883103\ttotal: 765ms\tremaining: 3.33s\n",
      "187:\tlearn: 0.6882880\ttotal: 769ms\tremaining: 3.32s\n",
      "188:\tlearn: 0.6882638\ttotal: 773ms\tremaining: 3.31s\n",
      "189:\tlearn: 0.6882337\ttotal: 777ms\tremaining: 3.31s\n",
      "190:\tlearn: 0.6881925\ttotal: 780ms\tremaining: 3.31s\n",
      "191:\tlearn: 0.6881728\ttotal: 785ms\tremaining: 3.3s\n",
      "192:\tlearn: 0.6881448\ttotal: 790ms\tremaining: 3.3s\n",
      "193:\tlearn: 0.6881215\ttotal: 795ms\tremaining: 3.3s\n",
      "194:\tlearn: 0.6880961\ttotal: 800ms\tremaining: 3.3s\n",
      "195:\tlearn: 0.6880616\ttotal: 804ms\tremaining: 3.3s\n",
      "196:\tlearn: 0.6880281\ttotal: 808ms\tremaining: 3.29s\n",
      "197:\tlearn: 0.6879907\ttotal: 812ms\tremaining: 3.29s\n",
      "198:\tlearn: 0.6879693\ttotal: 822ms\tremaining: 3.31s\n",
      "199:\tlearn: 0.6879362\ttotal: 827ms\tremaining: 3.31s\n",
      "200:\tlearn: 0.6879018\ttotal: 832ms\tremaining: 3.31s\n",
      "201:\tlearn: 0.6878809\ttotal: 837ms\tremaining: 3.31s\n",
      "202:\tlearn: 0.6878429\ttotal: 841ms\tremaining: 3.3s\n",
      "203:\tlearn: 0.6878058\ttotal: 844ms\tremaining: 3.29s\n",
      "204:\tlearn: 0.6877784\ttotal: 848ms\tremaining: 3.29s\n",
      "205:\tlearn: 0.6877497\ttotal: 852ms\tremaining: 3.28s\n",
      "206:\tlearn: 0.6877129\ttotal: 855ms\tremaining: 3.28s\n",
      "207:\tlearn: 0.6876770\ttotal: 859ms\tremaining: 3.27s\n",
      "208:\tlearn: 0.6876394\ttotal: 863ms\tremaining: 3.26s\n",
      "209:\tlearn: 0.6876013\ttotal: 867ms\tremaining: 3.26s\n",
      "210:\tlearn: 0.6875694\ttotal: 871ms\tremaining: 3.26s\n",
      "211:\tlearn: 0.6875471\ttotal: 874ms\tremaining: 3.25s\n",
      "212:\tlearn: 0.6875072\ttotal: 878ms\tremaining: 3.24s\n",
      "213:\tlearn: 0.6874687\ttotal: 881ms\tremaining: 3.24s\n",
      "214:\tlearn: 0.6874363\ttotal: 885ms\tremaining: 3.23s\n",
      "215:\tlearn: 0.6873987\ttotal: 889ms\tremaining: 3.23s\n",
      "216:\tlearn: 0.6873637\ttotal: 893ms\tremaining: 3.22s\n",
      "217:\tlearn: 0.6873264\ttotal: 896ms\tremaining: 3.21s\n",
      "218:\tlearn: 0.6872924\ttotal: 900ms\tremaining: 3.21s\n",
      "219:\tlearn: 0.6872687\ttotal: 904ms\tremaining: 3.21s\n",
      "220:\tlearn: 0.6872486\ttotal: 907ms\tremaining: 3.2s\n",
      "221:\tlearn: 0.6872116\ttotal: 911ms\tremaining: 3.19s\n",
      "222:\tlearn: 0.6871876\ttotal: 915ms\tremaining: 3.19s\n",
      "223:\tlearn: 0.6871570\ttotal: 918ms\tremaining: 3.18s\n",
      "224:\tlearn: 0.6871278\ttotal: 922ms\tremaining: 3.18s\n",
      "225:\tlearn: 0.6870885\ttotal: 926ms\tremaining: 3.17s\n",
      "226:\tlearn: 0.6870662\ttotal: 929ms\tremaining: 3.16s\n",
      "227:\tlearn: 0.6870311\ttotal: 934ms\tremaining: 3.16s\n",
      "228:\tlearn: 0.6869925\ttotal: 938ms\tremaining: 3.16s\n",
      "229:\tlearn: 0.6869610\ttotal: 943ms\tremaining: 3.15s\n",
      "230:\tlearn: 0.6869278\ttotal: 947ms\tremaining: 3.15s\n",
      "231:\tlearn: 0.6868840\ttotal: 952ms\tremaining: 3.15s\n",
      "232:\tlearn: 0.6868419\ttotal: 956ms\tremaining: 3.15s\n",
      "233:\tlearn: 0.6868196\ttotal: 959ms\tremaining: 3.14s\n",
      "234:\tlearn: 0.6867967\ttotal: 963ms\tremaining: 3.13s\n",
      "235:\tlearn: 0.6867635\ttotal: 967ms\tremaining: 3.13s\n",
      "236:\tlearn: 0.6867392\ttotal: 971ms\tremaining: 3.12s\n",
      "237:\tlearn: 0.6867034\ttotal: 974ms\tremaining: 3.12s\n",
      "238:\tlearn: 0.6866694\ttotal: 981ms\tremaining: 3.12s\n",
      "239:\tlearn: 0.6866388\ttotal: 988ms\tremaining: 3.13s\n",
      "240:\tlearn: 0.6866174\ttotal: 992ms\tremaining: 3.12s\n",
      "241:\tlearn: 0.6865983\ttotal: 996ms\tremaining: 3.12s\n",
      "242:\tlearn: 0.6865650\ttotal: 1000ms\tremaining: 3.11s\n",
      "243:\tlearn: 0.6865268\ttotal: 1s\tremaining: 3.11s\n",
      "244:\tlearn: 0.6864893\ttotal: 1.01s\tremaining: 3.1s\n",
      "245:\tlearn: 0.6864583\ttotal: 1.01s\tremaining: 3.1s\n",
      "246:\tlearn: 0.6864318\ttotal: 1.01s\tremaining: 3.09s\n",
      "247:\tlearn: 0.6864071\ttotal: 1.02s\tremaining: 3.09s\n",
      "248:\tlearn: 0.6863786\ttotal: 1.02s\tremaining: 3.08s\n",
      "249:\tlearn: 0.6863527\ttotal: 1.03s\tremaining: 3.08s\n",
      "250:\tlearn: 0.6863187\ttotal: 1.03s\tremaining: 3.08s\n",
      "251:\tlearn: 0.6862818\ttotal: 1.04s\tremaining: 3.08s\n",
      "252:\tlearn: 0.6862573\ttotal: 1.04s\tremaining: 3.08s\n",
      "253:\tlearn: 0.6862252\ttotal: 1.05s\tremaining: 3.07s\n",
      "254:\tlearn: 0.6861931\ttotal: 1.05s\tremaining: 3.07s\n",
      "255:\tlearn: 0.6861656\ttotal: 1.05s\tremaining: 3.06s\n",
      "256:\tlearn: 0.6861445\ttotal: 1.06s\tremaining: 3.06s\n",
      "257:\tlearn: 0.6861163\ttotal: 1.06s\tremaining: 3.05s\n",
      "258:\tlearn: 0.6860850\ttotal: 1.06s\tremaining: 3.05s\n",
      "259:\tlearn: 0.6860495\ttotal: 1.07s\tremaining: 3.04s\n",
      "260:\tlearn: 0.6860152\ttotal: 1.07s\tremaining: 3.04s\n",
      "261:\tlearn: 0.6859837\ttotal: 1.08s\tremaining: 3.03s\n",
      "262:\tlearn: 0.6859555\ttotal: 1.08s\tremaining: 3.03s\n",
      "263:\tlearn: 0.6859264\ttotal: 1.08s\tremaining: 3.02s\n",
      "264:\tlearn: 0.6858982\ttotal: 1.09s\tremaining: 3.02s\n",
      "265:\tlearn: 0.6858774\ttotal: 1.09s\tremaining: 3.01s\n",
      "266:\tlearn: 0.6858426\ttotal: 1.09s\tremaining: 3.01s\n",
      "267:\tlearn: 0.6858039\ttotal: 1.1s\tremaining: 3s\n",
      "268:\tlearn: 0.6857759\ttotal: 1.1s\tremaining: 3s\n",
      "269:\tlearn: 0.6857472\ttotal: 1.11s\tremaining: 2.99s\n",
      "270:\tlearn: 0.6857169\ttotal: 1.11s\tremaining: 2.99s\n",
      "271:\tlearn: 0.6856973\ttotal: 1.11s\tremaining: 2.98s\n",
      "272:\tlearn: 0.6856672\ttotal: 1.12s\tremaining: 2.98s\n",
      "273:\tlearn: 0.6856336\ttotal: 1.12s\tremaining: 2.97s\n",
      "274:\tlearn: 0.6856176\ttotal: 1.13s\tremaining: 2.96s\n",
      "275:\tlearn: 0.6855898\ttotal: 1.13s\tremaining: 2.96s\n",
      "276:\tlearn: 0.6855686\ttotal: 1.13s\tremaining: 2.96s\n",
      "277:\tlearn: 0.6855398\ttotal: 1.14s\tremaining: 2.95s\n",
      "278:\tlearn: 0.6854977\ttotal: 1.14s\tremaining: 2.95s\n",
      "279:\tlearn: 0.6854665\ttotal: 1.15s\tremaining: 2.94s\n",
      "280:\tlearn: 0.6854197\ttotal: 1.15s\tremaining: 2.94s\n",
      "281:\tlearn: 0.6853944\ttotal: 1.15s\tremaining: 2.94s\n",
      "282:\tlearn: 0.6853680\ttotal: 1.16s\tremaining: 2.93s\n",
      "283:\tlearn: 0.6853454\ttotal: 1.16s\tremaining: 2.92s\n",
      "284:\tlearn: 0.6853238\ttotal: 1.16s\tremaining: 2.92s\n",
      "285:\tlearn: 0.6852999\ttotal: 1.17s\tremaining: 2.92s\n",
      "286:\tlearn: 0.6852765\ttotal: 1.17s\tremaining: 2.91s\n",
      "287:\tlearn: 0.6852451\ttotal: 1.18s\tremaining: 2.91s\n",
      "288:\tlearn: 0.6852187\ttotal: 1.18s\tremaining: 2.9s\n",
      "289:\tlearn: 0.6851895\ttotal: 1.18s\tremaining: 2.9s\n",
      "290:\tlearn: 0.6851664\ttotal: 1.19s\tremaining: 2.9s\n",
      "291:\tlearn: 0.6851401\ttotal: 1.19s\tremaining: 2.89s\n",
      "292:\tlearn: 0.6851088\ttotal: 1.2s\tremaining: 2.88s\n",
      "293:\tlearn: 0.6850842\ttotal: 1.2s\tremaining: 2.88s\n",
      "294:\tlearn: 0.6850358\ttotal: 1.2s\tremaining: 2.88s\n",
      "295:\tlearn: 0.6850044\ttotal: 1.21s\tremaining: 2.87s\n",
      "296:\tlearn: 0.6849699\ttotal: 1.21s\tremaining: 2.87s\n",
      "297:\tlearn: 0.6849380\ttotal: 1.22s\tremaining: 2.86s\n",
      "298:\tlearn: 0.6849049\ttotal: 1.22s\tremaining: 2.86s\n",
      "299:\tlearn: 0.6848847\ttotal: 1.22s\tremaining: 2.85s\n",
      "300:\tlearn: 0.6848465\ttotal: 1.23s\tremaining: 2.85s\n",
      "301:\tlearn: 0.6848110\ttotal: 1.23s\tremaining: 2.85s\n",
      "302:\tlearn: 0.6847603\ttotal: 1.24s\tremaining: 2.84s\n",
      "303:\tlearn: 0.6847332\ttotal: 1.24s\tremaining: 2.84s\n",
      "304:\tlearn: 0.6847074\ttotal: 1.24s\tremaining: 2.84s\n",
      "305:\tlearn: 0.6846707\ttotal: 1.25s\tremaining: 2.83s\n",
      "306:\tlearn: 0.6846386\ttotal: 1.25s\tremaining: 2.83s\n",
      "307:\tlearn: 0.6846099\ttotal: 1.26s\tremaining: 2.83s\n",
      "308:\tlearn: 0.6845899\ttotal: 1.26s\tremaining: 2.82s\n",
      "309:\tlearn: 0.6845674\ttotal: 1.26s\tremaining: 2.82s\n",
      "310:\tlearn: 0.6845356\ttotal: 1.27s\tremaining: 2.81s\n",
      "311:\tlearn: 0.6845075\ttotal: 1.27s\tremaining: 2.81s\n",
      "312:\tlearn: 0.6844860\ttotal: 1.28s\tremaining: 2.8s\n",
      "313:\tlearn: 0.6844600\ttotal: 1.28s\tremaining: 2.8s\n",
      "314:\tlearn: 0.6844262\ttotal: 1.28s\tremaining: 2.79s\n",
      "315:\tlearn: 0.6843925\ttotal: 1.29s\tremaining: 2.79s\n",
      "316:\tlearn: 0.6843701\ttotal: 1.29s\tremaining: 2.79s\n",
      "317:\tlearn: 0.6843419\ttotal: 1.3s\tremaining: 2.78s\n",
      "318:\tlearn: 0.6843151\ttotal: 1.3s\tremaining: 2.78s\n",
      "319:\tlearn: 0.6842968\ttotal: 1.3s\tremaining: 2.77s\n",
      "320:\tlearn: 0.6842729\ttotal: 1.31s\tremaining: 2.77s\n",
      "321:\tlearn: 0.6842471\ttotal: 1.31s\tremaining: 2.76s\n",
      "322:\tlearn: 0.6842282\ttotal: 1.31s\tremaining: 2.76s\n",
      "323:\tlearn: 0.6842035\ttotal: 1.32s\tremaining: 2.75s\n",
      "324:\tlearn: 0.6841677\ttotal: 1.32s\tremaining: 2.75s\n",
      "325:\tlearn: 0.6841495\ttotal: 1.33s\tremaining: 2.74s\n",
      "326:\tlearn: 0.6841259\ttotal: 1.33s\tremaining: 2.74s\n",
      "327:\tlearn: 0.6841023\ttotal: 1.33s\tremaining: 2.74s\n",
      "328:\tlearn: 0.6840639\ttotal: 1.34s\tremaining: 2.73s\n",
      "329:\tlearn: 0.6840317\ttotal: 1.34s\tremaining: 2.73s\n",
      "330:\tlearn: 0.6840058\ttotal: 1.35s\tremaining: 2.72s\n",
      "331:\tlearn: 0.6839761\ttotal: 1.35s\tremaining: 2.72s\n",
      "332:\tlearn: 0.6839563\ttotal: 1.35s\tremaining: 2.71s\n",
      "333:\tlearn: 0.6839257\ttotal: 1.36s\tremaining: 2.71s\n",
      "334:\tlearn: 0.6838918\ttotal: 1.36s\tremaining: 2.71s\n",
      "335:\tlearn: 0.6838626\ttotal: 1.37s\tremaining: 2.7s\n",
      "336:\tlearn: 0.6838339\ttotal: 1.37s\tremaining: 2.7s\n",
      "337:\tlearn: 0.6837911\ttotal: 1.38s\tremaining: 2.7s\n",
      "338:\tlearn: 0.6837643\ttotal: 1.38s\tremaining: 2.69s\n",
      "339:\tlearn: 0.6837382\ttotal: 1.39s\tremaining: 2.69s\n",
      "340:\tlearn: 0.6837156\ttotal: 1.39s\tremaining: 2.68s\n",
      "341:\tlearn: 0.6836841\ttotal: 1.39s\tremaining: 2.68s\n",
      "342:\tlearn: 0.6836595\ttotal: 1.4s\tremaining: 2.67s\n",
      "343:\tlearn: 0.6836330\ttotal: 1.4s\tremaining: 2.67s\n",
      "344:\tlearn: 0.6836036\ttotal: 1.4s\tremaining: 2.67s\n",
      "345:\tlearn: 0.6835861\ttotal: 1.41s\tremaining: 2.66s\n",
      "346:\tlearn: 0.6835440\ttotal: 1.41s\tremaining: 2.65s\n",
      "347:\tlearn: 0.6835157\ttotal: 1.41s\tremaining: 2.65s\n",
      "348:\tlearn: 0.6834809\ttotal: 1.42s\tremaining: 2.65s\n",
      "349:\tlearn: 0.6834442\ttotal: 1.42s\tremaining: 2.64s\n",
      "350:\tlearn: 0.6834072\ttotal: 1.43s\tremaining: 2.64s\n",
      "351:\tlearn: 0.6833813\ttotal: 1.43s\tremaining: 2.63s\n",
      "352:\tlearn: 0.6833553\ttotal: 1.44s\tremaining: 2.63s\n",
      "353:\tlearn: 0.6833310\ttotal: 1.44s\tremaining: 2.63s\n",
      "354:\tlearn: 0.6833079\ttotal: 1.44s\tremaining: 2.62s\n",
      "355:\tlearn: 0.6832840\ttotal: 1.45s\tremaining: 2.62s\n",
      "356:\tlearn: 0.6832537\ttotal: 1.45s\tremaining: 2.61s\n",
      "357:\tlearn: 0.6832251\ttotal: 1.45s\tremaining: 2.6s\n",
      "358:\tlearn: 0.6832077\ttotal: 1.46s\tremaining: 2.6s\n",
      "359:\tlearn: 0.6831773\ttotal: 1.46s\tremaining: 2.6s\n",
      "360:\tlearn: 0.6831460\ttotal: 1.47s\tremaining: 2.59s\n",
      "361:\tlearn: 0.6831151\ttotal: 1.47s\tremaining: 2.59s\n",
      "362:\tlearn: 0.6830923\ttotal: 1.47s\tremaining: 2.58s\n",
      "363:\tlearn: 0.6830693\ttotal: 1.48s\tremaining: 2.58s\n",
      "364:\tlearn: 0.6830247\ttotal: 1.48s\tremaining: 2.57s\n",
      "365:\tlearn: 0.6830056\ttotal: 1.48s\tremaining: 2.57s\n",
      "366:\tlearn: 0.6829775\ttotal: 1.49s\tremaining: 2.56s\n",
      "367:\tlearn: 0.6829555\ttotal: 1.49s\tremaining: 2.56s\n",
      "368:\tlearn: 0.6829179\ttotal: 1.49s\tremaining: 2.56s\n",
      "369:\tlearn: 0.6828861\ttotal: 1.5s\tremaining: 2.55s\n",
      "370:\tlearn: 0.6828619\ttotal: 1.5s\tremaining: 2.55s\n",
      "371:\tlearn: 0.6828358\ttotal: 1.51s\tremaining: 2.54s\n",
      "372:\tlearn: 0.6828038\ttotal: 1.51s\tremaining: 2.54s\n",
      "373:\tlearn: 0.6827683\ttotal: 1.51s\tremaining: 2.53s\n",
      "374:\tlearn: 0.6827394\ttotal: 1.52s\tremaining: 2.53s\n",
      "375:\tlearn: 0.6827003\ttotal: 1.52s\tremaining: 2.53s\n",
      "376:\tlearn: 0.6826753\ttotal: 1.53s\tremaining: 2.52s\n",
      "377:\tlearn: 0.6826426\ttotal: 1.53s\tremaining: 2.52s\n",
      "378:\tlearn: 0.6826108\ttotal: 1.53s\tremaining: 2.51s\n",
      "379:\tlearn: 0.6825854\ttotal: 1.54s\tremaining: 2.51s\n",
      "380:\tlearn: 0.6825564\ttotal: 1.54s\tremaining: 2.5s\n",
      "381:\tlearn: 0.6825237\ttotal: 1.54s\tremaining: 2.5s\n",
      "382:\tlearn: 0.6824996\ttotal: 1.55s\tremaining: 2.5s\n",
      "383:\tlearn: 0.6824769\ttotal: 1.55s\tremaining: 2.49s\n",
      "384:\tlearn: 0.6824549\ttotal: 1.56s\tremaining: 2.49s\n",
      "385:\tlearn: 0.6824303\ttotal: 1.56s\tremaining: 2.48s\n",
      "386:\tlearn: 0.6823986\ttotal: 1.56s\tremaining: 2.48s\n",
      "387:\tlearn: 0.6823689\ttotal: 1.57s\tremaining: 2.47s\n",
      "388:\tlearn: 0.6823446\ttotal: 1.57s\tremaining: 2.47s\n",
      "389:\tlearn: 0.6823195\ttotal: 1.58s\tremaining: 2.46s\n",
      "390:\tlearn: 0.6823016\ttotal: 1.58s\tremaining: 2.46s\n",
      "391:\tlearn: 0.6822776\ttotal: 1.58s\tremaining: 2.46s\n",
      "392:\tlearn: 0.6822518\ttotal: 1.59s\tremaining: 2.46s\n",
      "393:\tlearn: 0.6822295\ttotal: 1.59s\tremaining: 2.45s\n",
      "394:\tlearn: 0.6821970\ttotal: 1.6s\tremaining: 2.45s\n",
      "395:\tlearn: 0.6821649\ttotal: 1.6s\tremaining: 2.44s\n",
      "396:\tlearn: 0.6821418\ttotal: 1.6s\tremaining: 2.44s\n",
      "397:\tlearn: 0.6821147\ttotal: 1.61s\tremaining: 2.43s\n",
      "398:\tlearn: 0.6820968\ttotal: 1.61s\tremaining: 2.43s\n",
      "399:\tlearn: 0.6820737\ttotal: 1.62s\tremaining: 2.42s\n",
      "400:\tlearn: 0.6820440\ttotal: 1.62s\tremaining: 2.42s\n",
      "401:\tlearn: 0.6820344\ttotal: 1.63s\tremaining: 2.42s\n",
      "402:\tlearn: 0.6820109\ttotal: 1.63s\tremaining: 2.41s\n",
      "403:\tlearn: 0.6819885\ttotal: 1.63s\tremaining: 2.41s\n",
      "404:\tlearn: 0.6819713\ttotal: 1.64s\tremaining: 2.41s\n",
      "405:\tlearn: 0.6819341\ttotal: 1.64s\tremaining: 2.4s\n",
      "406:\tlearn: 0.6819106\ttotal: 1.65s\tremaining: 2.4s\n",
      "407:\tlearn: 0.6818877\ttotal: 1.65s\tremaining: 2.39s\n",
      "408:\tlearn: 0.6818565\ttotal: 1.65s\tremaining: 2.39s\n",
      "409:\tlearn: 0.6818349\ttotal: 1.66s\tremaining: 2.38s\n",
      "410:\tlearn: 0.6818113\ttotal: 1.66s\tremaining: 2.38s\n",
      "411:\tlearn: 0.6817907\ttotal: 1.66s\tremaining: 2.37s\n",
      "412:\tlearn: 0.6817651\ttotal: 1.67s\tremaining: 2.37s\n",
      "413:\tlearn: 0.6817397\ttotal: 1.67s\tremaining: 2.37s\n",
      "414:\tlearn: 0.6817117\ttotal: 1.68s\tremaining: 2.36s\n",
      "415:\tlearn: 0.6816848\ttotal: 1.68s\tremaining: 2.36s\n",
      "416:\tlearn: 0.6816613\ttotal: 1.68s\tremaining: 2.35s\n",
      "417:\tlearn: 0.6816372\ttotal: 1.69s\tremaining: 2.35s\n",
      "418:\tlearn: 0.6816048\ttotal: 1.69s\tremaining: 2.34s\n",
      "419:\tlearn: 0.6815841\ttotal: 1.69s\tremaining: 2.34s\n",
      "420:\tlearn: 0.6815588\ttotal: 1.7s\tremaining: 2.33s\n",
      "421:\tlearn: 0.6815455\ttotal: 1.7s\tremaining: 2.33s\n",
      "422:\tlearn: 0.6815185\ttotal: 1.7s\tremaining: 2.33s\n",
      "423:\tlearn: 0.6814887\ttotal: 1.71s\tremaining: 2.32s\n",
      "424:\tlearn: 0.6814714\ttotal: 1.71s\tremaining: 2.32s\n",
      "425:\tlearn: 0.6814372\ttotal: 1.72s\tremaining: 2.31s\n",
      "426:\tlearn: 0.6814143\ttotal: 1.72s\tremaining: 2.31s\n",
      "427:\tlearn: 0.6813890\ttotal: 1.73s\tremaining: 2.31s\n",
      "428:\tlearn: 0.6813729\ttotal: 1.73s\tremaining: 2.3s\n",
      "429:\tlearn: 0.6813423\ttotal: 1.73s\tremaining: 2.3s\n",
      "430:\tlearn: 0.6813173\ttotal: 1.74s\tremaining: 2.29s\n",
      "431:\tlearn: 0.6812995\ttotal: 1.74s\tremaining: 2.29s\n",
      "432:\tlearn: 0.6812769\ttotal: 1.74s\tremaining: 2.28s\n",
      "433:\tlearn: 0.6812539\ttotal: 1.75s\tremaining: 2.28s\n",
      "434:\tlearn: 0.6812368\ttotal: 1.75s\tremaining: 2.27s\n",
      "435:\tlearn: 0.6812176\ttotal: 1.75s\tremaining: 2.27s\n",
      "436:\tlearn: 0.6811967\ttotal: 1.76s\tremaining: 2.26s\n",
      "437:\tlearn: 0.6811719\ttotal: 1.76s\tremaining: 2.26s\n",
      "438:\tlearn: 0.6811541\ttotal: 1.76s\tremaining: 2.25s\n",
      "439:\tlearn: 0.6811184\ttotal: 1.77s\tremaining: 2.25s\n",
      "440:\tlearn: 0.6810981\ttotal: 1.77s\tremaining: 2.25s\n",
      "441:\tlearn: 0.6810557\ttotal: 1.78s\tremaining: 2.24s\n",
      "442:\tlearn: 0.6810346\ttotal: 1.78s\tremaining: 2.24s\n",
      "443:\tlearn: 0.6810187\ttotal: 1.78s\tremaining: 2.23s\n",
      "444:\tlearn: 0.6809938\ttotal: 1.79s\tremaining: 2.23s\n",
      "445:\tlearn: 0.6809652\ttotal: 1.79s\tremaining: 2.23s\n",
      "446:\tlearn: 0.6809261\ttotal: 1.8s\tremaining: 2.22s\n",
      "447:\tlearn: 0.6808951\ttotal: 1.8s\tremaining: 2.22s\n",
      "448:\tlearn: 0.6808596\ttotal: 1.8s\tremaining: 2.21s\n",
      "449:\tlearn: 0.6808324\ttotal: 1.81s\tremaining: 2.21s\n",
      "450:\tlearn: 0.6808068\ttotal: 1.81s\tremaining: 2.21s\n",
      "451:\tlearn: 0.6807878\ttotal: 1.82s\tremaining: 2.21s\n",
      "452:\tlearn: 0.6807658\ttotal: 1.82s\tremaining: 2.2s\n",
      "453:\tlearn: 0.6807468\ttotal: 1.83s\tremaining: 2.2s\n",
      "454:\tlearn: 0.6807130\ttotal: 1.83s\tremaining: 2.19s\n",
      "455:\tlearn: 0.6806891\ttotal: 1.83s\tremaining: 2.19s\n",
      "456:\tlearn: 0.6806645\ttotal: 1.84s\tremaining: 2.18s\n",
      "457:\tlearn: 0.6806428\ttotal: 1.84s\tremaining: 2.18s\n",
      "458:\tlearn: 0.6806207\ttotal: 1.85s\tremaining: 2.18s\n",
      "459:\tlearn: 0.6805996\ttotal: 1.85s\tremaining: 2.17s\n",
      "460:\tlearn: 0.6805805\ttotal: 1.85s\tremaining: 2.17s\n",
      "461:\tlearn: 0.6805563\ttotal: 1.86s\tremaining: 2.16s\n",
      "462:\tlearn: 0.6805336\ttotal: 1.86s\tremaining: 2.16s\n",
      "463:\tlearn: 0.6805131\ttotal: 1.87s\tremaining: 2.15s\n",
      "464:\tlearn: 0.6804931\ttotal: 1.87s\tremaining: 2.15s\n",
      "465:\tlearn: 0.6804675\ttotal: 1.87s\tremaining: 2.15s\n",
      "466:\tlearn: 0.6804465\ttotal: 1.88s\tremaining: 2.14s\n",
      "467:\tlearn: 0.6804180\ttotal: 1.88s\tremaining: 2.14s\n",
      "468:\tlearn: 0.6803884\ttotal: 1.89s\tremaining: 2.13s\n",
      "469:\tlearn: 0.6803676\ttotal: 1.89s\tremaining: 2.13s\n",
      "470:\tlearn: 0.6803473\ttotal: 1.89s\tremaining: 2.13s\n",
      "471:\tlearn: 0.6803249\ttotal: 1.9s\tremaining: 2.12s\n",
      "472:\tlearn: 0.6802950\ttotal: 1.9s\tremaining: 2.12s\n",
      "473:\tlearn: 0.6802623\ttotal: 1.9s\tremaining: 2.11s\n",
      "474:\tlearn: 0.6802315\ttotal: 1.91s\tremaining: 2.11s\n",
      "475:\tlearn: 0.6802015\ttotal: 1.91s\tremaining: 2.1s\n",
      "476:\tlearn: 0.6801712\ttotal: 1.92s\tremaining: 2.1s\n",
      "477:\tlearn: 0.6801513\ttotal: 1.92s\tremaining: 2.1s\n",
      "478:\tlearn: 0.6801318\ttotal: 1.92s\tremaining: 2.09s\n",
      "479:\tlearn: 0.6801034\ttotal: 1.93s\tremaining: 2.09s\n",
      "480:\tlearn: 0.6800734\ttotal: 1.93s\tremaining: 2.08s\n",
      "481:\tlearn: 0.6800464\ttotal: 1.94s\tremaining: 2.08s\n",
      "482:\tlearn: 0.6800248\ttotal: 1.94s\tremaining: 2.08s\n",
      "483:\tlearn: 0.6799989\ttotal: 1.94s\tremaining: 2.07s\n",
      "484:\tlearn: 0.6799846\ttotal: 1.95s\tremaining: 2.07s\n",
      "485:\tlearn: 0.6799555\ttotal: 1.95s\tremaining: 2.06s\n",
      "486:\tlearn: 0.6799207\ttotal: 1.96s\tremaining: 2.06s\n",
      "487:\tlearn: 0.6798882\ttotal: 1.96s\tremaining: 2.06s\n",
      "488:\tlearn: 0.6798655\ttotal: 1.97s\tremaining: 2.06s\n",
      "489:\tlearn: 0.6798384\ttotal: 1.97s\tremaining: 2.05s\n",
      "490:\tlearn: 0.6798175\ttotal: 1.97s\tremaining: 2.05s\n",
      "491:\tlearn: 0.6798000\ttotal: 1.98s\tremaining: 2.04s\n",
      "492:\tlearn: 0.6797696\ttotal: 1.98s\tremaining: 2.04s\n",
      "493:\tlearn: 0.6797436\ttotal: 1.99s\tremaining: 2.04s\n",
      "494:\tlearn: 0.6797210\ttotal: 1.99s\tremaining: 2.03s\n",
      "495:\tlearn: 0.6796970\ttotal: 2s\tremaining: 2.03s\n",
      "496:\tlearn: 0.6796763\ttotal: 2s\tremaining: 2.02s\n",
      "497:\tlearn: 0.6796402\ttotal: 2s\tremaining: 2.02s\n",
      "498:\tlearn: 0.6796166\ttotal: 2.01s\tremaining: 2.02s\n",
      "499:\tlearn: 0.6795848\ttotal: 2.01s\tremaining: 2.01s\n",
      "500:\tlearn: 0.6795574\ttotal: 2.02s\tremaining: 2.01s\n",
      "501:\tlearn: 0.6795333\ttotal: 2.02s\tremaining: 2s\n",
      "502:\tlearn: 0.6795101\ttotal: 2.02s\tremaining: 2s\n",
      "503:\tlearn: 0.6794737\ttotal: 2.03s\tremaining: 2s\n",
      "504:\tlearn: 0.6794539\ttotal: 2.03s\tremaining: 1.99s\n",
      "505:\tlearn: 0.6794314\ttotal: 2.04s\tremaining: 1.99s\n",
      "506:\tlearn: 0.6794135\ttotal: 2.04s\tremaining: 1.98s\n",
      "507:\tlearn: 0.6793763\ttotal: 2.04s\tremaining: 1.98s\n",
      "508:\tlearn: 0.6793548\ttotal: 2.05s\tremaining: 1.98s\n",
      "509:\tlearn: 0.6793249\ttotal: 2.05s\tremaining: 1.97s\n",
      "510:\tlearn: 0.6792913\ttotal: 2.06s\tremaining: 1.97s\n",
      "511:\tlearn: 0.6792571\ttotal: 2.06s\tremaining: 1.96s\n",
      "512:\tlearn: 0.6792391\ttotal: 2.06s\tremaining: 1.96s\n",
      "513:\tlearn: 0.6792151\ttotal: 2.07s\tremaining: 1.95s\n",
      "514:\tlearn: 0.6791854\ttotal: 2.07s\tremaining: 1.95s\n",
      "515:\tlearn: 0.6791629\ttotal: 2.07s\tremaining: 1.95s\n",
      "516:\tlearn: 0.6791265\ttotal: 2.08s\tremaining: 1.94s\n",
      "517:\tlearn: 0.6790914\ttotal: 2.08s\tremaining: 1.94s\n",
      "518:\tlearn: 0.6790694\ttotal: 2.09s\tremaining: 1.93s\n",
      "519:\tlearn: 0.6790375\ttotal: 2.09s\tremaining: 1.93s\n",
      "520:\tlearn: 0.6790089\ttotal: 2.09s\tremaining: 1.92s\n",
      "521:\tlearn: 0.6789856\ttotal: 2.1s\tremaining: 1.92s\n",
      "522:\tlearn: 0.6789508\ttotal: 2.1s\tremaining: 1.92s\n",
      "523:\tlearn: 0.6789223\ttotal: 2.1s\tremaining: 1.91s\n",
      "524:\tlearn: 0.6789012\ttotal: 2.11s\tremaining: 1.91s\n",
      "525:\tlearn: 0.6788758\ttotal: 2.11s\tremaining: 1.9s\n",
      "526:\tlearn: 0.6788423\ttotal: 2.12s\tremaining: 1.9s\n",
      "527:\tlearn: 0.6788156\ttotal: 2.12s\tremaining: 1.9s\n",
      "528:\tlearn: 0.6787878\ttotal: 2.12s\tremaining: 1.89s\n",
      "529:\tlearn: 0.6787589\ttotal: 2.13s\tremaining: 1.89s\n",
      "530:\tlearn: 0.6787227\ttotal: 2.13s\tremaining: 1.88s\n",
      "531:\tlearn: 0.6787012\ttotal: 2.13s\tremaining: 1.88s\n",
      "532:\tlearn: 0.6786731\ttotal: 2.14s\tremaining: 1.87s\n",
      "533:\tlearn: 0.6786451\ttotal: 2.14s\tremaining: 1.87s\n",
      "534:\tlearn: 0.6786225\ttotal: 2.15s\tremaining: 1.87s\n",
      "535:\tlearn: 0.6786035\ttotal: 2.15s\tremaining: 1.86s\n",
      "536:\tlearn: 0.6785816\ttotal: 2.15s\tremaining: 1.86s\n",
      "537:\tlearn: 0.6785625\ttotal: 2.16s\tremaining: 1.85s\n",
      "538:\tlearn: 0.6785330\ttotal: 2.16s\tremaining: 1.85s\n",
      "539:\tlearn: 0.6785195\ttotal: 2.17s\tremaining: 1.85s\n",
      "540:\tlearn: 0.6784938\ttotal: 2.17s\tremaining: 1.84s\n",
      "541:\tlearn: 0.6784728\ttotal: 2.17s\tremaining: 1.84s\n",
      "542:\tlearn: 0.6784545\ttotal: 2.18s\tremaining: 1.83s\n",
      "543:\tlearn: 0.6784243\ttotal: 2.18s\tremaining: 1.83s\n",
      "544:\tlearn: 0.6783983\ttotal: 2.19s\tremaining: 1.83s\n",
      "545:\tlearn: 0.6783764\ttotal: 2.19s\tremaining: 1.82s\n",
      "546:\tlearn: 0.6783407\ttotal: 2.19s\tremaining: 1.82s\n",
      "547:\tlearn: 0.6783205\ttotal: 2.2s\tremaining: 1.81s\n",
      "548:\tlearn: 0.6782926\ttotal: 2.2s\tremaining: 1.81s\n",
      "549:\tlearn: 0.6782765\ttotal: 2.21s\tremaining: 1.81s\n",
      "550:\tlearn: 0.6782433\ttotal: 2.21s\tremaining: 1.8s\n",
      "551:\tlearn: 0.6782222\ttotal: 2.22s\tremaining: 1.8s\n",
      "552:\tlearn: 0.6781970\ttotal: 2.22s\tremaining: 1.79s\n",
      "553:\tlearn: 0.6781727\ttotal: 2.22s\tremaining: 1.79s\n",
      "554:\tlearn: 0.6781587\ttotal: 2.23s\tremaining: 1.79s\n",
      "555:\tlearn: 0.6781391\ttotal: 2.23s\tremaining: 1.78s\n",
      "556:\tlearn: 0.6780985\ttotal: 2.23s\tremaining: 1.78s\n",
      "557:\tlearn: 0.6780638\ttotal: 2.24s\tremaining: 1.77s\n",
      "558:\tlearn: 0.6780413\ttotal: 2.24s\tremaining: 1.77s\n",
      "559:\tlearn: 0.6780184\ttotal: 2.25s\tremaining: 1.76s\n",
      "560:\tlearn: 0.6779946\ttotal: 2.25s\tremaining: 1.76s\n",
      "561:\tlearn: 0.6779678\ttotal: 2.25s\tremaining: 1.76s\n",
      "562:\tlearn: 0.6779465\ttotal: 2.26s\tremaining: 1.75s\n",
      "563:\tlearn: 0.6779256\ttotal: 2.26s\tremaining: 1.75s\n",
      "564:\tlearn: 0.6779033\ttotal: 2.27s\tremaining: 1.75s\n",
      "565:\tlearn: 0.6778817\ttotal: 2.27s\tremaining: 1.74s\n",
      "566:\tlearn: 0.6778661\ttotal: 2.27s\tremaining: 1.74s\n",
      "567:\tlearn: 0.6778440\ttotal: 2.28s\tremaining: 1.73s\n",
      "568:\tlearn: 0.6778239\ttotal: 2.28s\tremaining: 1.73s\n",
      "569:\tlearn: 0.6777951\ttotal: 2.29s\tremaining: 1.72s\n",
      "570:\tlearn: 0.6777706\ttotal: 2.29s\tremaining: 1.72s\n",
      "571:\tlearn: 0.6777436\ttotal: 2.29s\tremaining: 1.72s\n",
      "572:\tlearn: 0.6777196\ttotal: 2.3s\tremaining: 1.71s\n",
      "573:\tlearn: 0.6776993\ttotal: 2.3s\tremaining: 1.71s\n",
      "574:\tlearn: 0.6776733\ttotal: 2.3s\tremaining: 1.7s\n",
      "575:\tlearn: 0.6776493\ttotal: 2.31s\tremaining: 1.7s\n",
      "576:\tlearn: 0.6776245\ttotal: 2.31s\tremaining: 1.7s\n",
      "577:\tlearn: 0.6775973\ttotal: 2.32s\tremaining: 1.69s\n",
      "578:\tlearn: 0.6775656\ttotal: 2.32s\tremaining: 1.69s\n",
      "579:\tlearn: 0.6775450\ttotal: 2.32s\tremaining: 1.68s\n",
      "580:\tlearn: 0.6775225\ttotal: 2.33s\tremaining: 1.68s\n",
      "581:\tlearn: 0.6774986\ttotal: 2.33s\tremaining: 1.67s\n",
      "582:\tlearn: 0.6774713\ttotal: 2.33s\tremaining: 1.67s\n",
      "583:\tlearn: 0.6774575\ttotal: 2.34s\tremaining: 1.67s\n",
      "584:\tlearn: 0.6774343\ttotal: 2.34s\tremaining: 1.66s\n",
      "585:\tlearn: 0.6774069\ttotal: 2.35s\tremaining: 1.66s\n",
      "586:\tlearn: 0.6773852\ttotal: 2.35s\tremaining: 1.65s\n",
      "587:\tlearn: 0.6773515\ttotal: 2.35s\tremaining: 1.65s\n",
      "588:\tlearn: 0.6773306\ttotal: 2.36s\tremaining: 1.65s\n",
      "589:\tlearn: 0.6773120\ttotal: 2.36s\tremaining: 1.64s\n",
      "590:\tlearn: 0.6772948\ttotal: 2.37s\tremaining: 1.64s\n",
      "591:\tlearn: 0.6772666\ttotal: 2.37s\tremaining: 1.63s\n",
      "592:\tlearn: 0.6772490\ttotal: 2.37s\tremaining: 1.63s\n",
      "593:\tlearn: 0.6772184\ttotal: 2.38s\tremaining: 1.63s\n",
      "594:\tlearn: 0.6771934\ttotal: 2.38s\tremaining: 1.62s\n",
      "595:\tlearn: 0.6771765\ttotal: 2.39s\tremaining: 1.62s\n",
      "596:\tlearn: 0.6771498\ttotal: 2.39s\tremaining: 1.61s\n",
      "597:\tlearn: 0.6771225\ttotal: 2.39s\tremaining: 1.61s\n",
      "598:\tlearn: 0.6771051\ttotal: 2.4s\tremaining: 1.6s\n",
      "599:\tlearn: 0.6770844\ttotal: 2.4s\tremaining: 1.6s\n",
      "600:\tlearn: 0.6770607\ttotal: 2.41s\tremaining: 1.6s\n",
      "601:\tlearn: 0.6770409\ttotal: 2.41s\tremaining: 1.59s\n",
      "602:\tlearn: 0.6770151\ttotal: 2.41s\tremaining: 1.59s\n",
      "603:\tlearn: 0.6769914\ttotal: 2.42s\tremaining: 1.58s\n",
      "604:\tlearn: 0.6769645\ttotal: 2.42s\tremaining: 1.58s\n",
      "605:\tlearn: 0.6769369\ttotal: 2.42s\tremaining: 1.58s\n",
      "606:\tlearn: 0.6769223\ttotal: 2.43s\tremaining: 1.57s\n",
      "607:\tlearn: 0.6769013\ttotal: 2.43s\tremaining: 1.57s\n",
      "608:\tlearn: 0.6768756\ttotal: 2.44s\tremaining: 1.56s\n",
      "609:\tlearn: 0.6768487\ttotal: 2.44s\tremaining: 1.56s\n",
      "610:\tlearn: 0.6768232\ttotal: 2.44s\tremaining: 1.56s\n",
      "611:\tlearn: 0.6768001\ttotal: 2.45s\tremaining: 1.55s\n",
      "612:\tlearn: 0.6767831\ttotal: 2.45s\tremaining: 1.55s\n",
      "613:\tlearn: 0.6767653\ttotal: 2.46s\tremaining: 1.54s\n",
      "614:\tlearn: 0.6767487\ttotal: 2.46s\tremaining: 1.54s\n",
      "615:\tlearn: 0.6767200\ttotal: 2.46s\tremaining: 1.53s\n",
      "616:\tlearn: 0.6766964\ttotal: 2.47s\tremaining: 1.53s\n",
      "617:\tlearn: 0.6766666\ttotal: 2.47s\tremaining: 1.53s\n",
      "618:\tlearn: 0.6766521\ttotal: 2.47s\tremaining: 1.52s\n",
      "619:\tlearn: 0.6766318\ttotal: 2.48s\tremaining: 1.52s\n",
      "620:\tlearn: 0.6766207\ttotal: 2.48s\tremaining: 1.51s\n",
      "621:\tlearn: 0.6765920\ttotal: 2.48s\tremaining: 1.51s\n",
      "622:\tlearn: 0.6765698\ttotal: 2.49s\tremaining: 1.51s\n",
      "623:\tlearn: 0.6765391\ttotal: 2.49s\tremaining: 1.5s\n",
      "624:\tlearn: 0.6765077\ttotal: 2.5s\tremaining: 1.5s\n",
      "625:\tlearn: 0.6764861\ttotal: 2.5s\tremaining: 1.49s\n",
      "626:\tlearn: 0.6764710\ttotal: 2.51s\tremaining: 1.49s\n",
      "627:\tlearn: 0.6764522\ttotal: 2.51s\tremaining: 1.49s\n",
      "628:\tlearn: 0.6764214\ttotal: 2.51s\tremaining: 1.48s\n",
      "629:\tlearn: 0.6764038\ttotal: 2.52s\tremaining: 1.48s\n",
      "630:\tlearn: 0.6763904\ttotal: 2.52s\tremaining: 1.48s\n",
      "631:\tlearn: 0.6763728\ttotal: 2.53s\tremaining: 1.47s\n",
      "632:\tlearn: 0.6763442\ttotal: 2.53s\tremaining: 1.47s\n",
      "633:\tlearn: 0.6763286\ttotal: 2.53s\tremaining: 1.46s\n",
      "634:\tlearn: 0.6762986\ttotal: 2.54s\tremaining: 1.46s\n",
      "635:\tlearn: 0.6762725\ttotal: 2.54s\tremaining: 1.45s\n",
      "636:\tlearn: 0.6762528\ttotal: 2.54s\tremaining: 1.45s\n",
      "637:\tlearn: 0.6762258\ttotal: 2.55s\tremaining: 1.45s\n",
      "638:\tlearn: 0.6761954\ttotal: 2.55s\tremaining: 1.44s\n",
      "639:\tlearn: 0.6761804\ttotal: 2.56s\tremaining: 1.44s\n",
      "640:\tlearn: 0.6761573\ttotal: 2.56s\tremaining: 1.43s\n",
      "641:\tlearn: 0.6761397\ttotal: 2.56s\tremaining: 1.43s\n",
      "642:\tlearn: 0.6761151\ttotal: 2.57s\tremaining: 1.43s\n",
      "643:\tlearn: 0.6761007\ttotal: 2.57s\tremaining: 1.42s\n",
      "644:\tlearn: 0.6760805\ttotal: 2.58s\tremaining: 1.42s\n",
      "645:\tlearn: 0.6760494\ttotal: 2.58s\tremaining: 1.41s\n",
      "646:\tlearn: 0.6760308\ttotal: 2.58s\tremaining: 1.41s\n",
      "647:\tlearn: 0.6760040\ttotal: 2.59s\tremaining: 1.41s\n",
      "648:\tlearn: 0.6759830\ttotal: 2.59s\tremaining: 1.4s\n",
      "649:\tlearn: 0.6759563\ttotal: 2.6s\tremaining: 1.4s\n",
      "650:\tlearn: 0.6759445\ttotal: 2.6s\tremaining: 1.39s\n",
      "651:\tlearn: 0.6759211\ttotal: 2.6s\tremaining: 1.39s\n",
      "652:\tlearn: 0.6758973\ttotal: 2.61s\tremaining: 1.39s\n",
      "653:\tlearn: 0.6758737\ttotal: 2.61s\tremaining: 1.38s\n",
      "654:\tlearn: 0.6758577\ttotal: 2.62s\tremaining: 1.38s\n",
      "655:\tlearn: 0.6758404\ttotal: 2.62s\tremaining: 1.37s\n",
      "656:\tlearn: 0.6758180\ttotal: 2.62s\tremaining: 1.37s\n",
      "657:\tlearn: 0.6758010\ttotal: 2.63s\tremaining: 1.36s\n",
      "658:\tlearn: 0.6757776\ttotal: 2.63s\tremaining: 1.36s\n",
      "659:\tlearn: 0.6757561\ttotal: 2.63s\tremaining: 1.36s\n",
      "660:\tlearn: 0.6757294\ttotal: 2.64s\tremaining: 1.35s\n",
      "661:\tlearn: 0.6757021\ttotal: 2.64s\tremaining: 1.35s\n",
      "662:\tlearn: 0.6756896\ttotal: 2.65s\tremaining: 1.34s\n",
      "663:\tlearn: 0.6756634\ttotal: 2.65s\tremaining: 1.34s\n",
      "664:\tlearn: 0.6756458\ttotal: 2.65s\tremaining: 1.34s\n",
      "665:\tlearn: 0.6756152\ttotal: 2.66s\tremaining: 1.33s\n",
      "666:\tlearn: 0.6755858\ttotal: 2.66s\tremaining: 1.33s\n",
      "667:\tlearn: 0.6755598\ttotal: 2.67s\tremaining: 1.32s\n",
      "668:\tlearn: 0.6755439\ttotal: 2.67s\tremaining: 1.32s\n",
      "669:\tlearn: 0.6755165\ttotal: 2.67s\tremaining: 1.32s\n",
      "670:\tlearn: 0.6754872\ttotal: 2.68s\tremaining: 1.31s\n",
      "671:\tlearn: 0.6754703\ttotal: 2.68s\tremaining: 1.31s\n",
      "672:\tlearn: 0.6754510\ttotal: 2.68s\tremaining: 1.3s\n",
      "673:\tlearn: 0.6754283\ttotal: 2.69s\tremaining: 1.3s\n",
      "674:\tlearn: 0.6754065\ttotal: 2.69s\tremaining: 1.3s\n",
      "675:\tlearn: 0.6753884\ttotal: 2.69s\tremaining: 1.29s\n",
      "676:\tlearn: 0.6753564\ttotal: 2.7s\tremaining: 1.29s\n",
      "677:\tlearn: 0.6753324\ttotal: 2.7s\tremaining: 1.28s\n",
      "678:\tlearn: 0.6753125\ttotal: 2.71s\tremaining: 1.28s\n",
      "679:\tlearn: 0.6752957\ttotal: 2.71s\tremaining: 1.28s\n",
      "680:\tlearn: 0.6752665\ttotal: 2.71s\tremaining: 1.27s\n",
      "681:\tlearn: 0.6752429\ttotal: 2.72s\tremaining: 1.27s\n",
      "682:\tlearn: 0.6752293\ttotal: 2.72s\tremaining: 1.26s\n",
      "683:\tlearn: 0.6752025\ttotal: 2.73s\tremaining: 1.26s\n",
      "684:\tlearn: 0.6751737\ttotal: 2.73s\tremaining: 1.25s\n",
      "685:\tlearn: 0.6751519\ttotal: 2.73s\tremaining: 1.25s\n",
      "686:\tlearn: 0.6751354\ttotal: 2.74s\tremaining: 1.25s\n",
      "687:\tlearn: 0.6751213\ttotal: 2.74s\tremaining: 1.24s\n",
      "688:\tlearn: 0.6751101\ttotal: 2.75s\tremaining: 1.24s\n",
      "689:\tlearn: 0.6750889\ttotal: 2.75s\tremaining: 1.23s\n",
      "690:\tlearn: 0.6750514\ttotal: 2.75s\tremaining: 1.23s\n",
      "691:\tlearn: 0.6750238\ttotal: 2.76s\tremaining: 1.23s\n",
      "692:\tlearn: 0.6749993\ttotal: 2.76s\tremaining: 1.22s\n",
      "693:\tlearn: 0.6749711\ttotal: 2.76s\tremaining: 1.22s\n",
      "694:\tlearn: 0.6749482\ttotal: 2.77s\tremaining: 1.22s\n",
      "695:\tlearn: 0.6749257\ttotal: 2.77s\tremaining: 1.21s\n",
      "696:\tlearn: 0.6749114\ttotal: 2.78s\tremaining: 1.21s\n",
      "697:\tlearn: 0.6748887\ttotal: 2.78s\tremaining: 1.2s\n",
      "698:\tlearn: 0.6748808\ttotal: 2.79s\tremaining: 1.2s\n",
      "699:\tlearn: 0.6748551\ttotal: 2.79s\tremaining: 1.2s\n",
      "700:\tlearn: 0.6748274\ttotal: 2.79s\tremaining: 1.19s\n",
      "701:\tlearn: 0.6748164\ttotal: 2.8s\tremaining: 1.19s\n",
      "702:\tlearn: 0.6747894\ttotal: 2.8s\tremaining: 1.18s\n",
      "703:\tlearn: 0.6747603\ttotal: 2.81s\tremaining: 1.18s\n",
      "704:\tlearn: 0.6747326\ttotal: 2.81s\tremaining: 1.18s\n",
      "705:\tlearn: 0.6747029\ttotal: 2.81s\tremaining: 1.17s\n",
      "706:\tlearn: 0.6746734\ttotal: 2.82s\tremaining: 1.17s\n",
      "707:\tlearn: 0.6746461\ttotal: 2.82s\tremaining: 1.16s\n",
      "708:\tlearn: 0.6746159\ttotal: 2.83s\tremaining: 1.16s\n",
      "709:\tlearn: 0.6745881\ttotal: 2.83s\tremaining: 1.16s\n",
      "710:\tlearn: 0.6745601\ttotal: 2.83s\tremaining: 1.15s\n",
      "711:\tlearn: 0.6745297\ttotal: 2.84s\tremaining: 1.15s\n",
      "712:\tlearn: 0.6745052\ttotal: 2.84s\tremaining: 1.14s\n",
      "713:\tlearn: 0.6744878\ttotal: 2.85s\tremaining: 1.14s\n",
      "714:\tlearn: 0.6744689\ttotal: 2.85s\tremaining: 1.14s\n",
      "715:\tlearn: 0.6744550\ttotal: 2.85s\tremaining: 1.13s\n",
      "716:\tlearn: 0.6744299\ttotal: 2.86s\tremaining: 1.13s\n",
      "717:\tlearn: 0.6744112\ttotal: 2.86s\tremaining: 1.12s\n",
      "718:\tlearn: 0.6743813\ttotal: 2.86s\tremaining: 1.12s\n",
      "719:\tlearn: 0.6743615\ttotal: 2.87s\tremaining: 1.11s\n",
      "720:\tlearn: 0.6743424\ttotal: 2.87s\tremaining: 1.11s\n",
      "721:\tlearn: 0.6743313\ttotal: 2.88s\tremaining: 1.11s\n",
      "722:\tlearn: 0.6743105\ttotal: 2.88s\tremaining: 1.1s\n",
      "723:\tlearn: 0.6742742\ttotal: 2.88s\tremaining: 1.1s\n",
      "724:\tlearn: 0.6742536\ttotal: 2.89s\tremaining: 1.09s\n",
      "725:\tlearn: 0.6742328\ttotal: 2.89s\tremaining: 1.09s\n",
      "726:\tlearn: 0.6742091\ttotal: 2.89s\tremaining: 1.09s\n",
      "727:\tlearn: 0.6741819\ttotal: 2.9s\tremaining: 1.08s\n",
      "728:\tlearn: 0.6741676\ttotal: 2.9s\tremaining: 1.08s\n",
      "729:\tlearn: 0.6741443\ttotal: 2.91s\tremaining: 1.07s\n",
      "730:\tlearn: 0.6741299\ttotal: 2.91s\tremaining: 1.07s\n",
      "731:\tlearn: 0.6740991\ttotal: 2.91s\tremaining: 1.07s\n",
      "732:\tlearn: 0.6740850\ttotal: 2.92s\tremaining: 1.06s\n",
      "733:\tlearn: 0.6740588\ttotal: 2.92s\tremaining: 1.06s\n",
      "734:\tlearn: 0.6740362\ttotal: 2.92s\tremaining: 1.05s\n",
      "735:\tlearn: 0.6740131\ttotal: 2.93s\tremaining: 1.05s\n",
      "736:\tlearn: 0.6739871\ttotal: 2.93s\tremaining: 1.05s\n",
      "737:\tlearn: 0.6739608\ttotal: 2.94s\tremaining: 1.04s\n",
      "738:\tlearn: 0.6739350\ttotal: 2.94s\tremaining: 1.04s\n",
      "739:\tlearn: 0.6739216\ttotal: 2.94s\tremaining: 1.03s\n",
      "740:\tlearn: 0.6739017\ttotal: 2.95s\tremaining: 1.03s\n",
      "741:\tlearn: 0.6738905\ttotal: 2.95s\tremaining: 1.03s\n",
      "742:\tlearn: 0.6738613\ttotal: 2.96s\tremaining: 1.02s\n",
      "743:\tlearn: 0.6738361\ttotal: 2.96s\tremaining: 1.02s\n",
      "744:\tlearn: 0.6738145\ttotal: 2.96s\tremaining: 1.01s\n",
      "745:\tlearn: 0.6738007\ttotal: 2.97s\tremaining: 1.01s\n",
      "746:\tlearn: 0.6737839\ttotal: 2.97s\tremaining: 1.01s\n",
      "747:\tlearn: 0.6737626\ttotal: 2.98s\tremaining: 1s\n",
      "748:\tlearn: 0.6737398\ttotal: 2.98s\tremaining: 999ms\n",
      "749:\tlearn: 0.6737227\ttotal: 2.98s\tremaining: 995ms\n",
      "750:\tlearn: 0.6736947\ttotal: 2.99s\tremaining: 991ms\n",
      "751:\tlearn: 0.6736808\ttotal: 2.99s\tremaining: 987ms\n",
      "752:\tlearn: 0.6736528\ttotal: 3s\tremaining: 983ms\n",
      "753:\tlearn: 0.6736246\ttotal: 3s\tremaining: 979ms\n",
      "754:\tlearn: 0.6736070\ttotal: 3s\tremaining: 975ms\n",
      "755:\tlearn: 0.6735901\ttotal: 3.01s\tremaining: 971ms\n",
      "756:\tlearn: 0.6735696\ttotal: 3.01s\tremaining: 967ms\n",
      "757:\tlearn: 0.6735436\ttotal: 3.02s\tremaining: 963ms\n",
      "758:\tlearn: 0.6735149\ttotal: 3.02s\tremaining: 959ms\n",
      "759:\tlearn: 0.6734918\ttotal: 3.02s\tremaining: 954ms\n",
      "760:\tlearn: 0.6734698\ttotal: 3.03s\tremaining: 950ms\n",
      "761:\tlearn: 0.6734551\ttotal: 3.03s\tremaining: 946ms\n",
      "762:\tlearn: 0.6734300\ttotal: 3.03s\tremaining: 942ms\n",
      "763:\tlearn: 0.6734112\ttotal: 3.04s\tremaining: 938ms\n",
      "764:\tlearn: 0.6733880\ttotal: 3.04s\tremaining: 934ms\n",
      "765:\tlearn: 0.6733649\ttotal: 3.04s\tremaining: 930ms\n",
      "766:\tlearn: 0.6733507\ttotal: 3.05s\tremaining: 926ms\n",
      "767:\tlearn: 0.6733323\ttotal: 3.05s\tremaining: 922ms\n",
      "768:\tlearn: 0.6733188\ttotal: 3.06s\tremaining: 918ms\n",
      "769:\tlearn: 0.6733004\ttotal: 3.06s\tremaining: 914ms\n",
      "770:\tlearn: 0.6732782\ttotal: 3.06s\tremaining: 910ms\n",
      "771:\tlearn: 0.6732621\ttotal: 3.07s\tremaining: 906ms\n",
      "772:\tlearn: 0.6732345\ttotal: 3.07s\tremaining: 902ms\n",
      "773:\tlearn: 0.6732135\ttotal: 3.08s\tremaining: 898ms\n",
      "774:\tlearn: 0.6732036\ttotal: 3.08s\tremaining: 894ms\n",
      "775:\tlearn: 0.6731780\ttotal: 3.08s\tremaining: 890ms\n",
      "776:\tlearn: 0.6731489\ttotal: 3.09s\tremaining: 886ms\n",
      "777:\tlearn: 0.6731321\ttotal: 3.09s\tremaining: 882ms\n",
      "778:\tlearn: 0.6731123\ttotal: 3.1s\tremaining: 878ms\n",
      "779:\tlearn: 0.6730880\ttotal: 3.1s\tremaining: 874ms\n",
      "780:\tlearn: 0.6730638\ttotal: 3.1s\tremaining: 870ms\n",
      "781:\tlearn: 0.6730385\ttotal: 3.11s\tremaining: 866ms\n",
      "782:\tlearn: 0.6730200\ttotal: 3.11s\tremaining: 862ms\n",
      "783:\tlearn: 0.6729986\ttotal: 3.11s\tremaining: 858ms\n",
      "784:\tlearn: 0.6729761\ttotal: 3.12s\tremaining: 854ms\n",
      "785:\tlearn: 0.6729496\ttotal: 3.12s\tremaining: 850ms\n",
      "786:\tlearn: 0.6729330\ttotal: 3.13s\tremaining: 846ms\n",
      "787:\tlearn: 0.6729119\ttotal: 3.13s\tremaining: 842ms\n",
      "788:\tlearn: 0.6728890\ttotal: 3.13s\tremaining: 838ms\n",
      "789:\tlearn: 0.6728681\ttotal: 3.14s\tremaining: 834ms\n",
      "790:\tlearn: 0.6728567\ttotal: 3.14s\tremaining: 830ms\n",
      "791:\tlearn: 0.6728363\ttotal: 3.14s\tremaining: 826ms\n",
      "792:\tlearn: 0.6728027\ttotal: 3.15s\tremaining: 822ms\n",
      "793:\tlearn: 0.6727697\ttotal: 3.15s\tremaining: 818ms\n",
      "794:\tlearn: 0.6727508\ttotal: 3.15s\tremaining: 814ms\n",
      "795:\tlearn: 0.6727315\ttotal: 3.16s\tremaining: 810ms\n",
      "796:\tlearn: 0.6727084\ttotal: 3.16s\tremaining: 806ms\n",
      "797:\tlearn: 0.6726897\ttotal: 3.17s\tremaining: 802ms\n",
      "798:\tlearn: 0.6726609\ttotal: 3.17s\tremaining: 798ms\n",
      "799:\tlearn: 0.6726322\ttotal: 3.18s\tremaining: 794ms\n",
      "800:\tlearn: 0.6726010\ttotal: 3.18s\tremaining: 790ms\n",
      "801:\tlearn: 0.6725719\ttotal: 3.19s\tremaining: 786ms\n",
      "802:\tlearn: 0.6725544\ttotal: 3.19s\tremaining: 782ms\n",
      "803:\tlearn: 0.6725318\ttotal: 3.19s\tremaining: 779ms\n",
      "804:\tlearn: 0.6725170\ttotal: 3.2s\tremaining: 774ms\n",
      "805:\tlearn: 0.6724845\ttotal: 3.2s\tremaining: 770ms\n",
      "806:\tlearn: 0.6724671\ttotal: 3.21s\tremaining: 767ms\n",
      "807:\tlearn: 0.6724492\ttotal: 3.21s\tremaining: 763ms\n",
      "808:\tlearn: 0.6724257\ttotal: 3.21s\tremaining: 758ms\n",
      "809:\tlearn: 0.6724041\ttotal: 3.22s\tremaining: 754ms\n",
      "810:\tlearn: 0.6723851\ttotal: 3.22s\tremaining: 750ms\n",
      "811:\tlearn: 0.6723439\ttotal: 3.22s\tremaining: 746ms\n",
      "812:\tlearn: 0.6723136\ttotal: 3.23s\tremaining: 743ms\n",
      "813:\tlearn: 0.6722901\ttotal: 3.23s\tremaining: 739ms\n",
      "814:\tlearn: 0.6722615\ttotal: 3.23s\tremaining: 734ms\n",
      "815:\tlearn: 0.6722479\ttotal: 3.24s\tremaining: 730ms\n",
      "816:\tlearn: 0.6722268\ttotal: 3.24s\tremaining: 726ms\n",
      "817:\tlearn: 0.6722110\ttotal: 3.25s\tremaining: 722ms\n",
      "818:\tlearn: 0.6721938\ttotal: 3.25s\tremaining: 718ms\n",
      "819:\tlearn: 0.6721736\ttotal: 3.25s\tremaining: 714ms\n",
      "820:\tlearn: 0.6721600\ttotal: 3.26s\tremaining: 710ms\n",
      "821:\tlearn: 0.6721401\ttotal: 3.26s\tremaining: 706ms\n",
      "822:\tlearn: 0.6721228\ttotal: 3.26s\tremaining: 702ms\n",
      "823:\tlearn: 0.6721093\ttotal: 3.27s\tremaining: 698ms\n",
      "824:\tlearn: 0.6720934\ttotal: 3.27s\tremaining: 694ms\n",
      "825:\tlearn: 0.6720755\ttotal: 3.28s\tremaining: 690ms\n",
      "826:\tlearn: 0.6720515\ttotal: 3.28s\tremaining: 686ms\n",
      "827:\tlearn: 0.6720232\ttotal: 3.28s\tremaining: 682ms\n",
      "828:\tlearn: 0.6720054\ttotal: 3.29s\tremaining: 678ms\n",
      "829:\tlearn: 0.6719919\ttotal: 3.29s\tremaining: 674ms\n",
      "830:\tlearn: 0.6719729\ttotal: 3.3s\tremaining: 670ms\n",
      "831:\tlearn: 0.6719520\ttotal: 3.3s\tremaining: 666ms\n",
      "832:\tlearn: 0.6719340\ttotal: 3.3s\tremaining: 662ms\n",
      "833:\tlearn: 0.6718978\ttotal: 3.31s\tremaining: 658ms\n",
      "834:\tlearn: 0.6718747\ttotal: 3.31s\tremaining: 654ms\n",
      "835:\tlearn: 0.6718495\ttotal: 3.31s\tremaining: 650ms\n",
      "836:\tlearn: 0.6718265\ttotal: 3.32s\tremaining: 646ms\n",
      "837:\tlearn: 0.6717976\ttotal: 3.32s\tremaining: 642ms\n",
      "838:\tlearn: 0.6717777\ttotal: 3.33s\tremaining: 638ms\n",
      "839:\tlearn: 0.6717640\ttotal: 3.33s\tremaining: 634ms\n",
      "840:\tlearn: 0.6717492\ttotal: 3.33s\tremaining: 630ms\n",
      "841:\tlearn: 0.6717231\ttotal: 3.34s\tremaining: 626ms\n",
      "842:\tlearn: 0.6717013\ttotal: 3.34s\tremaining: 622ms\n",
      "843:\tlearn: 0.6716910\ttotal: 3.35s\tremaining: 619ms\n",
      "844:\tlearn: 0.6716667\ttotal: 3.35s\tremaining: 615ms\n",
      "845:\tlearn: 0.6716471\ttotal: 3.36s\tremaining: 611ms\n",
      "846:\tlearn: 0.6716212\ttotal: 3.36s\tremaining: 607ms\n",
      "847:\tlearn: 0.6715976\ttotal: 3.37s\tremaining: 603ms\n",
      "848:\tlearn: 0.6715696\ttotal: 3.37s\tremaining: 599ms\n",
      "849:\tlearn: 0.6715429\ttotal: 3.37s\tremaining: 596ms\n",
      "850:\tlearn: 0.6715211\ttotal: 3.38s\tremaining: 592ms\n",
      "851:\tlearn: 0.6715026\ttotal: 3.38s\tremaining: 588ms\n",
      "852:\tlearn: 0.6714810\ttotal: 3.39s\tremaining: 584ms\n",
      "853:\tlearn: 0.6714619\ttotal: 3.39s\tremaining: 580ms\n",
      "854:\tlearn: 0.6714371\ttotal: 3.4s\tremaining: 576ms\n",
      "855:\tlearn: 0.6714213\ttotal: 3.4s\tremaining: 572ms\n",
      "856:\tlearn: 0.6713977\ttotal: 3.4s\tremaining: 568ms\n",
      "857:\tlearn: 0.6713723\ttotal: 3.41s\tremaining: 564ms\n",
      "858:\tlearn: 0.6713579\ttotal: 3.41s\tremaining: 560ms\n",
      "859:\tlearn: 0.6713457\ttotal: 3.41s\tremaining: 556ms\n",
      "860:\tlearn: 0.6713241\ttotal: 3.42s\tremaining: 552ms\n",
      "861:\tlearn: 0.6712971\ttotal: 3.42s\tremaining: 548ms\n",
      "862:\tlearn: 0.6712809\ttotal: 3.42s\tremaining: 544ms\n",
      "863:\tlearn: 0.6712568\ttotal: 3.43s\tremaining: 540ms\n",
      "864:\tlearn: 0.6712414\ttotal: 3.43s\tremaining: 536ms\n",
      "865:\tlearn: 0.6712235\ttotal: 3.44s\tremaining: 532ms\n",
      "866:\tlearn: 0.6712096\ttotal: 3.44s\tremaining: 528ms\n",
      "867:\tlearn: 0.6711843\ttotal: 3.44s\tremaining: 524ms\n",
      "868:\tlearn: 0.6711662\ttotal: 3.45s\tremaining: 520ms\n",
      "869:\tlearn: 0.6711528\ttotal: 3.45s\tremaining: 516ms\n",
      "870:\tlearn: 0.6711398\ttotal: 3.46s\tremaining: 512ms\n",
      "871:\tlearn: 0.6711177\ttotal: 3.46s\tremaining: 508ms\n",
      "872:\tlearn: 0.6710978\ttotal: 3.46s\tremaining: 504ms\n",
      "873:\tlearn: 0.6710766\ttotal: 3.47s\tremaining: 500ms\n",
      "874:\tlearn: 0.6710626\ttotal: 3.47s\tremaining: 496ms\n",
      "875:\tlearn: 0.6710457\ttotal: 3.47s\tremaining: 492ms\n",
      "876:\tlearn: 0.6710206\ttotal: 3.48s\tremaining: 488ms\n",
      "877:\tlearn: 0.6710048\ttotal: 3.48s\tremaining: 484ms\n",
      "878:\tlearn: 0.6709893\ttotal: 3.49s\tremaining: 480ms\n",
      "879:\tlearn: 0.6709750\ttotal: 3.49s\tremaining: 476ms\n",
      "880:\tlearn: 0.6709469\ttotal: 3.5s\tremaining: 472ms\n",
      "881:\tlearn: 0.6709323\ttotal: 3.5s\tremaining: 468ms\n",
      "882:\tlearn: 0.6709061\ttotal: 3.5s\tremaining: 464ms\n",
      "883:\tlearn: 0.6708872\ttotal: 3.51s\tremaining: 460ms\n",
      "884:\tlearn: 0.6708689\ttotal: 3.51s\tremaining: 456ms\n",
      "885:\tlearn: 0.6708470\ttotal: 3.52s\tremaining: 452ms\n",
      "886:\tlearn: 0.6708266\ttotal: 3.52s\tremaining: 448ms\n",
      "887:\tlearn: 0.6708015\ttotal: 3.52s\tremaining: 444ms\n",
      "888:\tlearn: 0.6707757\ttotal: 3.53s\tremaining: 440ms\n",
      "889:\tlearn: 0.6707533\ttotal: 3.53s\tremaining: 436ms\n",
      "890:\tlearn: 0.6707361\ttotal: 3.53s\tremaining: 432ms\n",
      "891:\tlearn: 0.6707178\ttotal: 3.54s\tremaining: 428ms\n",
      "892:\tlearn: 0.6706980\ttotal: 3.54s\tremaining: 424ms\n",
      "893:\tlearn: 0.6706793\ttotal: 3.55s\tremaining: 421ms\n",
      "894:\tlearn: 0.6706603\ttotal: 3.56s\tremaining: 417ms\n",
      "895:\tlearn: 0.6706366\ttotal: 3.56s\tremaining: 413ms\n",
      "896:\tlearn: 0.6706253\ttotal: 3.56s\tremaining: 409ms\n",
      "897:\tlearn: 0.6706013\ttotal: 3.57s\tremaining: 405ms\n",
      "898:\tlearn: 0.6705866\ttotal: 3.57s\tremaining: 401ms\n",
      "899:\tlearn: 0.6705567\ttotal: 3.58s\tremaining: 397ms\n",
      "900:\tlearn: 0.6705333\ttotal: 3.58s\tremaining: 393ms\n",
      "901:\tlearn: 0.6705177\ttotal: 3.58s\tremaining: 390ms\n",
      "902:\tlearn: 0.6704963\ttotal: 3.59s\tremaining: 386ms\n",
      "903:\tlearn: 0.6704743\ttotal: 3.59s\tremaining: 382ms\n",
      "904:\tlearn: 0.6704550\ttotal: 3.6s\tremaining: 378ms\n",
      "905:\tlearn: 0.6704324\ttotal: 3.6s\tremaining: 374ms\n",
      "906:\tlearn: 0.6704148\ttotal: 3.6s\tremaining: 370ms\n",
      "907:\tlearn: 0.6703864\ttotal: 3.61s\tremaining: 366ms\n",
      "908:\tlearn: 0.6703683\ttotal: 3.61s\tremaining: 362ms\n",
      "909:\tlearn: 0.6703574\ttotal: 3.62s\tremaining: 358ms\n",
      "910:\tlearn: 0.6703346\ttotal: 3.62s\tremaining: 354ms\n",
      "911:\tlearn: 0.6703104\ttotal: 3.62s\tremaining: 350ms\n",
      "912:\tlearn: 0.6702994\ttotal: 3.63s\tremaining: 346ms\n",
      "913:\tlearn: 0.6702686\ttotal: 3.63s\tremaining: 342ms\n",
      "914:\tlearn: 0.6702476\ttotal: 3.63s\tremaining: 338ms\n",
      "915:\tlearn: 0.6702410\ttotal: 3.64s\tremaining: 334ms\n",
      "916:\tlearn: 0.6702285\ttotal: 3.64s\tremaining: 330ms\n",
      "917:\tlearn: 0.6702041\ttotal: 3.65s\tremaining: 326ms\n",
      "918:\tlearn: 0.6701856\ttotal: 3.65s\tremaining: 322ms\n",
      "919:\tlearn: 0.6701535\ttotal: 3.65s\tremaining: 318ms\n",
      "920:\tlearn: 0.6701344\ttotal: 3.66s\tremaining: 314ms\n",
      "921:\tlearn: 0.6701072\ttotal: 3.66s\tremaining: 310ms\n",
      "922:\tlearn: 0.6700956\ttotal: 3.66s\tremaining: 306ms\n",
      "923:\tlearn: 0.6700767\ttotal: 3.67s\tremaining: 302ms\n",
      "924:\tlearn: 0.6700540\ttotal: 3.67s\tremaining: 298ms\n",
      "925:\tlearn: 0.6700225\ttotal: 3.68s\tremaining: 294ms\n",
      "926:\tlearn: 0.6700024\ttotal: 3.68s\tremaining: 290ms\n",
      "927:\tlearn: 0.6699879\ttotal: 3.68s\tremaining: 286ms\n",
      "928:\tlearn: 0.6699727\ttotal: 3.69s\tremaining: 282ms\n",
      "929:\tlearn: 0.6699496\ttotal: 3.69s\tremaining: 278ms\n",
      "930:\tlearn: 0.6699290\ttotal: 3.69s\tremaining: 274ms\n",
      "931:\tlearn: 0.6699083\ttotal: 3.7s\tremaining: 270ms\n",
      "932:\tlearn: 0.6698859\ttotal: 3.7s\tremaining: 266ms\n",
      "933:\tlearn: 0.6698495\ttotal: 3.71s\tremaining: 262ms\n",
      "934:\tlearn: 0.6698355\ttotal: 3.71s\tremaining: 258ms\n",
      "935:\tlearn: 0.6698137\ttotal: 3.71s\tremaining: 254ms\n",
      "936:\tlearn: 0.6697883\ttotal: 3.72s\tremaining: 250ms\n",
      "937:\tlearn: 0.6697775\ttotal: 3.72s\tremaining: 246ms\n",
      "938:\tlearn: 0.6697547\ttotal: 3.73s\tremaining: 242ms\n",
      "939:\tlearn: 0.6697336\ttotal: 3.73s\tremaining: 238ms\n",
      "940:\tlearn: 0.6697067\ttotal: 3.73s\tremaining: 234ms\n",
      "941:\tlearn: 0.6696825\ttotal: 3.74s\tremaining: 230ms\n",
      "942:\tlearn: 0.6696620\ttotal: 3.74s\tremaining: 226ms\n",
      "943:\tlearn: 0.6696454\ttotal: 3.75s\tremaining: 222ms\n",
      "944:\tlearn: 0.6696341\ttotal: 3.75s\tremaining: 219ms\n",
      "945:\tlearn: 0.6696042\ttotal: 3.76s\tremaining: 215ms\n",
      "946:\tlearn: 0.6695866\ttotal: 3.76s\tremaining: 211ms\n",
      "947:\tlearn: 0.6695611\ttotal: 3.77s\tremaining: 207ms\n",
      "948:\tlearn: 0.6695473\ttotal: 3.77s\tremaining: 203ms\n",
      "949:\tlearn: 0.6695339\ttotal: 3.78s\tremaining: 199ms\n",
      "950:\tlearn: 0.6695262\ttotal: 3.78s\tremaining: 195ms\n",
      "951:\tlearn: 0.6694963\ttotal: 3.79s\tremaining: 191ms\n",
      "952:\tlearn: 0.6694799\ttotal: 3.79s\tremaining: 187ms\n",
      "953:\tlearn: 0.6694663\ttotal: 3.8s\tremaining: 183ms\n",
      "954:\tlearn: 0.6694454\ttotal: 3.8s\tremaining: 179ms\n",
      "955:\tlearn: 0.6694268\ttotal: 3.81s\tremaining: 175ms\n",
      "956:\tlearn: 0.6694099\ttotal: 3.81s\tremaining: 171ms\n",
      "957:\tlearn: 0.6693920\ttotal: 3.82s\tremaining: 167ms\n",
      "958:\tlearn: 0.6693710\ttotal: 3.82s\tremaining: 163ms\n",
      "959:\tlearn: 0.6693402\ttotal: 3.83s\tremaining: 159ms\n",
      "960:\tlearn: 0.6693213\ttotal: 3.83s\tremaining: 155ms\n",
      "961:\tlearn: 0.6693015\ttotal: 3.83s\tremaining: 152ms\n",
      "962:\tlearn: 0.6692886\ttotal: 3.84s\tremaining: 148ms\n",
      "963:\tlearn: 0.6692707\ttotal: 3.85s\tremaining: 144ms\n",
      "964:\tlearn: 0.6692381\ttotal: 3.85s\tremaining: 140ms\n",
      "965:\tlearn: 0.6692202\ttotal: 3.85s\tremaining: 136ms\n",
      "966:\tlearn: 0.6692067\ttotal: 3.86s\tremaining: 132ms\n",
      "967:\tlearn: 0.6691927\ttotal: 3.86s\tremaining: 128ms\n",
      "968:\tlearn: 0.6691631\ttotal: 3.87s\tremaining: 124ms\n",
      "969:\tlearn: 0.6691359\ttotal: 3.87s\tremaining: 120ms\n",
      "970:\tlearn: 0.6691188\ttotal: 3.87s\tremaining: 116ms\n",
      "971:\tlearn: 0.6691040\ttotal: 3.88s\tremaining: 112ms\n",
      "972:\tlearn: 0.6690719\ttotal: 3.88s\tremaining: 108ms\n",
      "973:\tlearn: 0.6690543\ttotal: 3.89s\tremaining: 104ms\n",
      "974:\tlearn: 0.6690373\ttotal: 3.89s\tremaining: 99.8ms\n",
      "975:\tlearn: 0.6690233\ttotal: 3.9s\tremaining: 95.8ms\n",
      "976:\tlearn: 0.6690099\ttotal: 3.9s\tremaining: 91.9ms\n",
      "977:\tlearn: 0.6689905\ttotal: 3.9s\tremaining: 87.9ms\n",
      "978:\tlearn: 0.6689599\ttotal: 3.91s\tremaining: 83.9ms\n",
      "979:\tlearn: 0.6689403\ttotal: 3.91s\tremaining: 79.9ms\n",
      "980:\tlearn: 0.6689061\ttotal: 3.92s\tremaining: 75.9ms\n",
      "981:\tlearn: 0.6688872\ttotal: 3.92s\tremaining: 71.9ms\n",
      "982:\tlearn: 0.6688573\ttotal: 3.92s\tremaining: 67.9ms\n",
      "983:\tlearn: 0.6688436\ttotal: 3.93s\tremaining: 63.9ms\n",
      "984:\tlearn: 0.6688217\ttotal: 3.93s\tremaining: 59.9ms\n",
      "985:\tlearn: 0.6688028\ttotal: 3.94s\tremaining: 55.9ms\n",
      "986:\tlearn: 0.6687843\ttotal: 3.94s\tremaining: 51.9ms\n",
      "987:\tlearn: 0.6687552\ttotal: 3.94s\tremaining: 47.9ms\n",
      "988:\tlearn: 0.6687377\ttotal: 3.95s\tremaining: 43.9ms\n",
      "989:\tlearn: 0.6687222\ttotal: 3.95s\tremaining: 39.9ms\n",
      "990:\tlearn: 0.6687055\ttotal: 3.96s\tremaining: 35.9ms\n",
      "991:\tlearn: 0.6686841\ttotal: 3.96s\tremaining: 32ms\n",
      "992:\tlearn: 0.6686703\ttotal: 3.97s\tremaining: 28ms\n",
      "993:\tlearn: 0.6686492\ttotal: 3.97s\tremaining: 24ms\n",
      "994:\tlearn: 0.6686243\ttotal: 3.97s\tremaining: 20ms\n",
      "995:\tlearn: 0.6686054\ttotal: 3.98s\tremaining: 16ms\n",
      "996:\tlearn: 0.6685885\ttotal: 3.98s\tremaining: 12ms\n",
      "997:\tlearn: 0.6685670\ttotal: 3.99s\tremaining: 7.99ms\n",
      "998:\tlearn: 0.6685415\ttotal: 3.99s\tremaining: 3.99ms\n",
      "999:\tlearn: 0.6685189\ttotal: 4s\tremaining: 0us\n",
      "Training Gradient Boosting...\n",
      "                     Model   Recall  Training Time (s)\n",
      "16       Gradient Boosting  0.50280           4.580560\n",
      "2                      SVC  0.50185         162.183089\n",
      "8   Support Vector Machine  0.50185         163.710214\n",
      "5              Naive Bayes  0.50135           0.030716\n",
      "6      Logistic Regression  0.49995           0.037074\n",
      "10   Bagging Decision Tree  0.49990           1.819273\n",
      "7     K-Nearest Neighbours  0.49980           0.760046\n",
      "4           MLP Classifier  0.49935           5.573252\n",
      "15               Cat Boost  0.49875           4.421322\n",
      "1           XGB Classifier  0.49860           0.357527\n",
      "14                XG Boost  0.49860           0.164598\n",
      "0            Random Forest  0.49780           1.617429\n",
      "12  Random Forest Ensemble  0.49740           1.462992\n",
      "11   Boosted Decision Tree  0.49730           3.270785\n",
      "13   Voting Classification  0.49645         155.370629\n",
      "9            Decision Tree  0.49540           0.336921\n",
      "3         Isolation Forest  0.01405           0.273254\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "\n",
    "## Model Training Automation\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_jobs=-1),  # Set n_jobs to -1 for parallel training\n",
    "    'XGB Classifier': XGBClassifier(n_jobs=-1),  # Set n_jobs to -1 for parallel training\n",
    "    'SVC': SVC(),\n",
    "    'Isolation Forest': IsolationForest(n_jobs=-1),  # Set n_jobs to -1 for parallel training\n",
    "    'MLP Classifier': MLPClassifier(activation='relu'),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'K-Nearest Neighbours': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Bagging Decision Tree': BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_jobs=-1),  # Set n_jobs to -1 for parallel training\n",
    "    'Boosted Decision Tree': AdaBoostClassifier(base_estimator=DecisionTreeClassifier()),  # Remove n_jobs\n",
    "    'Random Forest Ensemble': RandomForestClassifier(n_jobs=-1),  # Set n_jobs to -1 for parallel training\n",
    "    'Voting Classification': VotingClassifier(estimators=[('rf', RandomForestClassifier(n_jobs=-1)), ('xgb', XGBClassifier(n_jobs=-1)), ('svc', SVC())]),\n",
    "    'XG Boost': XGBClassifier(n_jobs=-1),  # Set n_jobs to -1 for parallel training\n",
    "    'Cat Boost': CatBoostClassifier(thread_count=-1),  # Set thread_count to -1 for parallel training\n",
    "    'Gradient Boosting': GradientBoostingClassifier()  # Remove n_jobs\n",
    "}\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, models):\n",
    "    report = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        \n",
    "        # Start measuring time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict Testing data\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Get recall score for test data prediction\n",
    "        recall = recall_score(y_test, y_test_pred, average='micro')\n",
    "        \n",
    "        # Calculate elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        report.append({'Model': model_name, 'Recall': recall, 'Training Time (s)': elapsed_time})\n",
    "\n",
    "    # Create a DataFrame from the report\n",
    "    report_df = pd.DataFrame(report)\n",
    "\n",
    "    # Sort the report by Recall in descending order\n",
    "    report_df = report_df.sort_values(by='Recall', ascending=False)\n",
    "\n",
    "    return report_df\n",
    "\n",
    "# Use transformed features for model training and evaluation\n",
    "recall_report = evaluate_model(X_train_transformed, y_train, X_test_transformed, y_test, models)\n",
    "print(recall_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Training Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.50280</td>\n",
       "      <td>4.580560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.50185</td>\n",
       "      <td>162.183089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.50185</td>\n",
       "      <td>163.710214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.50135</td>\n",
       "      <td>0.030716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.49995</td>\n",
       "      <td>0.037074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bagging Decision Tree</td>\n",
       "      <td>0.49990</td>\n",
       "      <td>1.819273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>K-Nearest Neighbours</td>\n",
       "      <td>0.49980</td>\n",
       "      <td>0.760046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP Classifier</td>\n",
       "      <td>0.49935</td>\n",
       "      <td>5.573252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Cat Boost</td>\n",
       "      <td>0.49875</td>\n",
       "      <td>4.421322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGB Classifier</td>\n",
       "      <td>0.49860</td>\n",
       "      <td>0.357527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XG Boost</td>\n",
       "      <td>0.49860</td>\n",
       "      <td>0.164598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.49780</td>\n",
       "      <td>1.617429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Random Forest Ensemble</td>\n",
       "      <td>0.49740</td>\n",
       "      <td>1.462992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Boosted Decision Tree</td>\n",
       "      <td>0.49730</td>\n",
       "      <td>3.270785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Voting Classification</td>\n",
       "      <td>0.49645</td>\n",
       "      <td>155.370629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.49540</td>\n",
       "      <td>0.336921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Isolation Forest</td>\n",
       "      <td>0.01405</td>\n",
       "      <td>0.273254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model   Recall  Training Time (s)\n",
       "16       Gradient Boosting  0.50280           4.580560\n",
       "2                      SVC  0.50185         162.183089\n",
       "8   Support Vector Machine  0.50185         163.710214\n",
       "5              Naive Bayes  0.50135           0.030716\n",
       "6      Logistic Regression  0.49995           0.037074\n",
       "10   Bagging Decision Tree  0.49990           1.819273\n",
       "7     K-Nearest Neighbours  0.49980           0.760046\n",
       "4           MLP Classifier  0.49935           5.573252\n",
       "15               Cat Boost  0.49875           4.421322\n",
       "1           XGB Classifier  0.49860           0.357527\n",
       "14                XG Boost  0.49860           0.164598\n",
       "0            Random Forest  0.49780           1.617429\n",
       "12  Random Forest Ensemble  0.49740           1.462992\n",
       "11   Boosted Decision Tree  0.49730           3.270785\n",
       "13   Voting Classification  0.49645         155.370629\n",
       "9            Decision Tree  0.49540           0.336921\n",
       "3         Isolation Forest  0.01405           0.273254"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight: Selecting the Optimal Churn Prediction Model**\n",
    "\n",
    "In our pursuit of building an effective churn prediction model, we recognize the importance of identifying the best-performing algorithm that strikes a balance between high recall and reasonable training time. To achieve this, we have conducted an initial evaluation of various machine learning algorithms on our dataset. Among these models, we have shortlisted the top 5 performers based on their recall scores, which are crucial for our churn prediction problem. These top 5 models are Gradient Boosting, SVC, Support Vector Machine, Naive Bayes, and Logistic Regression.\n",
    "\n",
    "Here are the key insights and our plan moving forward:\n",
    "\n",
    "1. **Recall-Centric Approach**: Our primary goal in churn prediction is to identify as many true churned customers as possible to minimize revenue loss. Therefore, we prioritize recall, as it helps capture a significant portion of actual churn cases.\n",
    "\n",
    "2. **Top 5 Models**: After the initial evaluation, we have identified the following top 5 models with their respective recall scores: Gradient Boosting, SVC, Support Vector Machine, Naive Bayes, and Logistic Regression. These models have shown promise in effectively identifying churned customers.\n",
    "\n",
    "3. **Hyperparameter Tuning**: To further enhance the performance of these models, we plan to conduct hyperparameter tuning. This process involves optimizing the settings and parameters of each model to maximize their predictive power. By fine-tuning these algorithms, we aim to squeeze out their full potential for our specific dataset.\n",
    "\n",
    "4. **Model Selection**: Once we have tuned the hyperparameters of these top 5 models, we will evaluate their performance again. This will allow us to select the best-performing model that meets our high recall requirements while considering practical training times.\n",
    "\n",
    "5. **Balancing Recall and Efficiency**: While recall is our primary focus, we also acknowledge the importance of training time efficiency. We aim to strike the right balance between recall and training time, ensuring that the selected model can be deployed efficiently in a real-world scenario.\n",
    "\n",
    "6. **Iterative Process**: Model selection and hyperparameter tuning are often iterative processes. We will fine-tune the models, evaluate their performance, and make adjustments as necessary until we identify the model that best aligns with our churn prediction objectives.\n",
    "\n",
    "In conclusion, our approach involves rigorously evaluating, tuning, and selecting the optimal churn prediction model from the top 5 performers. This process will empower us to build a highly effective churn prediction system that minimizes revenue loss while being computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.225 total time=  10.8s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.145 total time=  11.3s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.154 total time=  11.4s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=4, n_estimators=100;, score=0.208 total time=  14.3s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=4, n_estimators=100;, score=0.207 total time=  14.8s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=4, n_estimators=100;, score=0.223 total time=  14.9s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.234 total time=  22.3s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.224 total time=  22.8s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.252 total time=  24.0s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.279 total time=  33.2s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.257 total time=  33.9s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.286 total time=  35.0s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.235 total time=  17.9s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.244 total time=  18.6s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=4, n_estimators=200;, score=0.275 total time=  30.8s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=4, n_estimators=200;, score=0.259 total time=  30.6s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.242 total time=  18.8s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=4, n_estimators=200;, score=0.306 total time=  31.7s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.413 total time=  10.9s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.397 total time=  10.8s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.392 total time=  11.6s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=4, n_estimators=300;, score=0.342 total time=  43.7s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=4, n_estimators=300;, score=0.308 total time=  44.7s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=4, n_estimators=300;, score=0.300 total time=  46.8s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.284 total time=  33.8s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.305 total time=  35.3s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.316 total time=  38.3s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.434 total time=  22.6s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.426 total time=  22.6s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.442 total time=  23.4s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=0.441 total time=  14.6s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=0.422 total time=  14.5s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=0.430 total time=  14.4s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.451 total time=  31.0s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.450 total time=  34.7s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.440 total time=  33.4s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=300;, score=0.322 total time=  55.1s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=300;, score=0.351 total time=  54.3s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=300;, score=0.353 total time=  55.1s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=4, n_estimators=200;, score=0.457 total time=  26.3s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=4, n_estimators=200;, score=0.448 total time=  28.2s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=4, n_estimators=200;, score=0.445 total time=  29.2s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.452 total time=  18.4s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.424 total time=  17.5s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.443 total time=  17.7s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.434 total time=  11.8s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=4, n_estimators=300;, score=0.469 total time=  40.8s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.444 total time=  11.4s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.428 total time=  11.6s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=4, n_estimators=300;, score=0.459 total time=  42.4s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=4, n_estimators=300;, score=0.461 total time=  42.0s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.459 total time=  34.0s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.480 total time=  34.7s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.475 total time=  35.4s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.453 total time=  24.3s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.458 total time=  22.4s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.448 total time=  21.5s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=4, n_estimators=100;, score=0.460 total time=  16.4s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=4, n_estimators=100;, score=0.453 total time=  17.2s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=4, n_estimators=100;, score=0.460 total time=  16.3s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=300;, score=0.495 total time=  54.6s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=300;, score=0.464 total time=  51.7s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=3, n_estimators=300;, score=0.466 total time=  36.1s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=3, n_estimators=300;, score=0.466 total time=  33.3s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=300;, score=0.490 total time=  55.2s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=3, n_estimators=300;, score=0.458 total time=  33.2s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=4, n_estimators=200;, score=0.481 total time=  25.5s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=4, n_estimators=200;, score=0.477 total time=  26.4s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=4, n_estimators=200;, score=0.472 total time=  27.0s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=5, n_estimators=100;, score=0.481 total time=  15.9s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=5, n_estimators=100;, score=0.460 total time=  16.8s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=5, n_estimators=100;, score=0.454 total time=  16.1s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=4, n_estimators=300;, score=0.479 total time=  32.4s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=4, n_estimators=300;, score=0.495 total time=  34.0s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=4, n_estimators=300;, score=0.479 total time=  35.5s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=5, n_estimators=200;, score=0.474 total time=  22.9s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=5, n_estimators=200;, score=0.487 total time=  24.9s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=5, n_estimators=200;, score=0.476 total time=  25.7s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=5, n_estimators=300;, score=0.483 total time=  22.3s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=5, n_estimators=300;, score=0.486 total time=  24.6s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=5, n_estimators=300;, score=0.482 total time=  23.6s\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV 3/3] END .C=0.1, gamma=scale, kernel=linear;, score=0.600 total time= 6.6min\n",
      "[CV 2/3] END .C=0.1, gamma=scale, kernel=linear;, score=0.398 total time= 6.7min\n",
      "[CV 1/3] END ..C=0.1, gamma=auto, kernel=linear;, score=0.407 total time= 6.7min\n",
      "[CV 1/3] END .C=0.1, gamma=scale, kernel=linear;, score=0.407 total time= 6.7min\n",
      "[CV 2/3] END ..C=0.1, gamma=auto, kernel=linear;, score=0.398 total time= 6.7min\n",
      "[CV 3/3] END ..C=0.1, gamma=auto, kernel=linear;, score=0.600 total time= 6.8min\n",
      "[CV 2/3] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.383 total time= 6.9min\n",
      "[CV 3/3] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.442 total time= 7.1min\n",
      "[CV 2/3] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.326 total time= 7.1min\n",
      "[CV 1/3] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.337 total time= 7.2min\n",
      "[CV 3/3] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.437 total time= 7.3min\n",
      "[CV 1/3] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.429 total time= 7.3min\n",
      "[CV 2/3] END ...C=1, gamma=scale, kernel=linear;, score=0.398 total time= 6.6min\n",
      "[CV 1/3] END ...C=1, gamma=scale, kernel=linear;, score=0.407 total time= 6.7min\n",
      "[CV 3/3] END ...C=1, gamma=scale, kernel=linear;, score=0.600 total time= 6.7min\n",
      "[CV 1/3] END ....C=1, gamma=auto, kernel=linear;, score=0.407 total time= 6.7min\n",
      "[CV 2/3] END ......C=1, gamma=scale, kernel=rbf;, score=0.379 total time= 6.9min\n",
      "[CV 2/3] END ....C=1, gamma=auto, kernel=linear;, score=0.398 total time= 6.5min\n",
      "[CV 3/3] END ....C=1, gamma=auto, kernel=linear;, score=0.600 total time= 6.6min\n",
      "[CV 1/3] END ......C=1, gamma=scale, kernel=rbf;, score=0.472 total time= 7.2min\n",
      "[CV 3/3] END ......C=1, gamma=scale, kernel=rbf;, score=0.435 total time= 7.2min\n",
      "[CV 1/3] END .......C=1, gamma=auto, kernel=rbf;, score=0.460 total time= 7.1min\n",
      "[CV 2/3] END .......C=1, gamma=auto, kernel=rbf;, score=0.338 total time= 6.9min\n",
      "[CV 3/3] END .......C=1, gamma=auto, kernel=rbf;, score=0.436 total time= 7.1min\n",
      "[CV 2/3] END .....C=10, gamma=scale, kernel=rbf;, score=0.437 total time= 7.7min\n",
      "[CV 1/3] END .....C=10, gamma=scale, kernel=rbf;, score=0.473 total time= 7.8min\n",
      "[CV 3/3] END .....C=10, gamma=scale, kernel=rbf;, score=0.451 total time= 7.8min\n",
      "[CV 2/3] END ......C=10, gamma=auto, kernel=rbf;, score=0.367 total time= 7.3min\n",
      "[CV 3/3] END ......C=10, gamma=auto, kernel=rbf;, score=0.417 total time= 7.1min\n",
      "[CV 1/3] END ......C=10, gamma=auto, kernel=rbf;, score=0.455 total time= 7.3min\n",
      "[CV 1/3] END ..C=10, gamma=scale, kernel=linear;, score=0.407 total time= 8.5min\n",
      "[CV 2/3] END ..C=10, gamma=scale, kernel=linear;, score=0.398 total time= 8.5min\n",
      "[CV 1/3] END ...C=10, gamma=auto, kernel=linear;, score=0.407 total time= 8.3min\n",
      "[CV 2/3] END ...C=10, gamma=auto, kernel=linear;, score=0.398 total time= 8.2min\n",
      "[CV 3/3] END ..C=10, gamma=scale, kernel=linear;, score=0.600 total time= 9.2min\n",
      "[CV 3/3] END ...C=10, gamma=auto, kernel=linear;, score=0.600 total time= 8.8min\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV 1/3] END ..C=0.1, gamma=auto, kernel=linear;, score=0.407 total time= 6.6min\n",
      "[CV 1/3] END .C=0.1, gamma=scale, kernel=linear;, score=0.407 total time= 6.7min\n",
      "[CV 3/3] END ..C=0.1, gamma=auto, kernel=linear;, score=0.600 total time= 6.8min\n",
      "[CV 2/3] END ..C=0.1, gamma=auto, kernel=linear;, score=0.398 total time= 6.8min\n",
      "[CV 3/3] END .C=0.1, gamma=scale, kernel=linear;, score=0.600 total time= 6.9min\n",
      "[CV 2/3] END .C=0.1, gamma=scale, kernel=linear;, score=0.398 total time= 6.9min\n",
      "[CV 2/3] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.383 total time= 7.0min\n",
      "[CV 1/3] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.337 total time= 7.2min\n",
      "[CV 2/3] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.326 total time= 7.3min\n",
      "[CV 1/3] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.429 total time= 7.4min\n",
      "[CV 3/3] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.442 total time= 7.4min\n",
      "[CV 3/3] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.437 total time= 7.5min\n",
      "[CV 3/3] END ...C=1, gamma=scale, kernel=linear;, score=0.600 total time= 6.5min\n",
      "[CV 2/3] END ...C=1, gamma=scale, kernel=linear;, score=0.398 total time= 6.6min\n",
      "[CV 1/3] END ...C=1, gamma=scale, kernel=linear;, score=0.407 total time= 6.7min\n",
      "[CV 1/3] END ....C=1, gamma=auto, kernel=linear;, score=0.407 total time= 6.5min\n",
      "[CV 2/3] END ....C=1, gamma=auto, kernel=linear;, score=0.398 total time= 6.4min\n",
      "[CV 2/3] END ......C=1, gamma=scale, kernel=rbf;, score=0.379 total time= 6.9min\n",
      "[CV 3/3] END ......C=1, gamma=scale, kernel=rbf;, score=0.435 total time= 6.9min\n",
      "[CV 3/3] END ....C=1, gamma=auto, kernel=linear;, score=0.600 total time= 6.5min\n",
      "[CV 1/3] END ......C=1, gamma=scale, kernel=rbf;, score=0.472 total time= 7.1min\n",
      "[CV 2/3] END .......C=1, gamma=auto, kernel=rbf;, score=0.338 total time= 6.8min\n",
      "[CV 1/3] END .......C=1, gamma=auto, kernel=rbf;, score=0.460 total time= 6.8min\n",
      "[CV 3/3] END .......C=1, gamma=auto, kernel=rbf;, score=0.436 total time= 6.8min\n",
      "[CV 1/3] END .....C=10, gamma=scale, kernel=rbf;, score=0.473 total time= 7.3min\n",
      "[CV 2/3] END .....C=10, gamma=scale, kernel=rbf;, score=0.437 total time= 7.3min\n",
      "[CV 2/3] END ......C=10, gamma=auto, kernel=rbf;, score=0.367 total time= 6.7min\n",
      "[CV 1/3] END ......C=10, gamma=auto, kernel=rbf;, score=0.455 total time= 6.8min\n",
      "[CV 3/3] END .....C=10, gamma=scale, kernel=rbf;, score=0.451 total time= 7.2min\n",
      "[CV 3/3] END ......C=10, gamma=auto, kernel=rbf;, score=0.417 total time= 6.7min\n",
      "[CV 1/3] END ..C=10, gamma=scale, kernel=linear;, score=0.407 total time= 8.2min\n",
      "[CV 2/3] END ..C=10, gamma=scale, kernel=linear;, score=0.398 total time= 8.2min\n",
      "[CV 1/3] END ...C=10, gamma=auto, kernel=linear;, score=0.407 total time= 7.7min\n",
      "[CV 2/3] END ...C=10, gamma=auto, kernel=linear;, score=0.398 total time= 7.8min\n",
      "[CV 3/3] END ..C=10, gamma=scale, kernel=linear;, score=0.600 total time= 8.7min\n",
      "[CV 3/3] END ...C=10, gamma=auto, kernel=linear;, score=0.600 total time= 8.4min\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV 1/3] END ...............var_smoothing=1e-09;, score=0.471 total time=   0.0s\n",
      "[CV 3/3] END ...............var_smoothing=1e-08;, score=0.600 total time=   0.0s\n",
      "[CV 2/3] END ...............var_smoothing=1e-08;, score=0.419 total time=   0.1s\n",
      "[CV 1/3] END ...............var_smoothing=1e-07;, score=0.471 total time=   0.0s\n",
      "[CV 2/3] END ...............var_smoothing=1e-07;, score=0.419 total time=   0.1s\n",
      "[CV 3/3] END ...............var_smoothing=1e-07;, score=0.600 total time=   0.1s\n",
      "[CV 2/3] END ...............var_smoothing=1e-09;, score=0.419 total time=   0.1s\n",
      "[CV 3/3] END ...............var_smoothing=1e-09;, score=0.600 total time=   0.1s\n",
      "[CV 1/3] END ...............var_smoothing=1e-08;, score=0.471 total time=   0.1s\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV 2/3] END C=0.1, penalty=l1, solver=liblinear;, score=0.389 total time=   0.1s\n",
      "[CV 1/3] END C=0.1, penalty=l1, solver=liblinear;, score=0.346 total time=   0.1s\n",
      "[CV 3/3] END C=0.1, penalty=l1, solver=liblinear;, score=0.552 total time=   0.1s\n",
      "[CV 2/3] END .C=1, penalty=l1, solver=liblinear;, score=0.391 total time=   0.2s\n",
      "[CV 1/3] END .C=1, penalty=l1, solver=liblinear;, score=0.374 total time=   0.2s\n",
      "[CV 3/3] END .C=1, penalty=l1, solver=liblinear;, score=0.543 total time=   0.3s\n",
      "[CV 3/3] END ....C=0.1, penalty=l1, solver=saga;, score=0.556 total time=   1.0s\n",
      "[CV 1/3] END ....C=0.1, penalty=l1, solver=saga;, score=0.346 total time=   1.1s\n",
      "[CV 1/3] END .C=1, penalty=l2, solver=liblinear;, score=0.377 total time=   0.1s\n",
      "[CV 1/3] END ......C=1, penalty=l1, solver=saga;, score=0.374 total time=   0.9s\n",
      "[CV 3/3] END C=0.1, penalty=l2, solver=liblinear;, score=0.542 total time=   0.1s[CV 1/3] END C=0.1, penalty=l2, solver=liblinear;, score=0.376 total time=   0.1s\n",
      "\n",
      "[CV 2/3] END .C=1, penalty=l2, solver=liblinear;, score=0.391 total time=   0.2s\n",
      "[CV 2/3] END ....C=0.1, penalty=l1, solver=saga;, score=0.374 total time=   1.3s\n",
      "[CV 3/3] END .C=1, penalty=l2, solver=liblinear;, score=0.542 total time=   0.1s\n",
      "[CV 2/3] END C=0.1, penalty=l2, solver=liblinear;, score=0.391 total time=   0.2s\n",
      "[CV 2/3] END ......C=1, penalty=l1, solver=saga;, score=0.390 total time=   1.1s\n",
      "[CV 3/3] END ......C=1, penalty=l1, solver=saga;, score=0.543 total time=   1.1s\n",
      "[CV 2/3] END ....C=0.1, penalty=l2, solver=saga;, score=0.391 total time=   0.9s\n",
      "[CV 1/3] END C=10, penalty=l2, solver=liblinear;, score=0.377 total time=   0.1s\n",
      "[CV 3/3] END ....C=0.1, penalty=l2, solver=saga;, score=0.542 total time=   1.2s\n",
      "[CV 2/3] END C=10, penalty=l2, solver=liblinear;, score=0.391 total time=   0.1s\n",
      "[CV 1/3] END ....C=0.1, penalty=l2, solver=saga;, score=0.376 total time=   1.2s\n",
      "[CV 3/3] END C=10, penalty=l2, solver=liblinear;, score=0.542 total time=   0.1s\n",
      "[CV 1/3] END .....C=10, penalty=l1, solver=saga;, score=0.376 total time=   1.5s\n",
      "[CV 3/3] END .....C=10, penalty=l1, solver=saga;, score=0.542 total time=   1.3s\n",
      "[CV 2/3] END .....C=10, penalty=l1, solver=saga;, score=0.391 total time=   1.5s\n",
      "[CV 1/3] END ......C=1, penalty=l2, solver=saga;, score=0.377 total time=   1.9s\n",
      "[CV 2/3] END ......C=1, penalty=l2, solver=saga;, score=0.391 total time=   2.0s\n",
      "[CV 3/3] END ......C=1, penalty=l2, solver=saga;, score=0.542 total time=   2.0s\n",
      "[CV 2/3] END C=10, penalty=l1, solver=liblinear;, score=0.391 total time=   2.1s\n",
      "[CV 1/3] END C=10, penalty=l1, solver=liblinear;, score=0.376 total time=   2.3s\n",
      "[CV 3/3] END C=10, penalty=l1, solver=liblinear;, score=0.542 total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milan/anaconda3/envs/churn/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/milan/anaconda3/envs/churn/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/milan/anaconda3/envs/churn/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END .....C=10, penalty=l2, solver=saga;, score=0.391 total time=   1.7s\n",
      "[CV 1/3] END .....C=10, penalty=l2, solver=saga;, score=0.377 total time=   1.8s\n",
      "[CV 3/3] END .....C=10, penalty=l2, solver=saga;, score=0.542 total time=   1.7s\n",
      "                    Model   Recall  Training Time (s)\n",
      "3             Naive Bayes  0.50135           0.030604\n",
      "1                     SVC  0.50115         110.922408\n",
      "2  Support Vector Machine  0.50115         112.039930\n",
      "4     Logistic Regression  0.50010           0.061136\n",
      "0       Gradient Boosting  0.49975          17.474802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the top 5 models based on initial recall scores\n",
    "top_5_models = {\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "}\n",
    "\n",
    "# Define hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7],\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store the tuned models and their recall scores\n",
    "tuned_models_recall = {}\n",
    "\n",
    "# Loop through the top 5 models, perform hyperparameter tuning, and retain the best model\n",
    "for model_name, model in top_5_models.items():\n",
    "    # Initialize GridSearchCV with the model, hyperparameter grid, and scoring method (recall)\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], scoring='recall', cv=3, n_jobs=-1, verbose=3)  # Set verbose to 3\n",
    "    \n",
    "    # Fit GridSearchCV to the data\n",
    "    grid_search.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    # Get the best model with hyperparameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Retrain the best model on the full training data\n",
    "    best_model.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_test_pred = best_model.predict(X_test_transformed)\n",
    "    \n",
    "    # Calculate recall score for test data prediction\n",
    "    recall = recall_score(y_test, y_test_pred, average='micro')\n",
    "    \n",
    "    # Store the best model and its recall score\n",
    "    tuned_models_recall[model_name] = {\n",
    "        'Model': model_name,\n",
    "        'Recall': recall,\n",
    "        'Training Time (s)': grid_search.refit_time_\n",
    "    }\n",
    "\n",
    "# Print the tuned models and their recall scores\n",
    "tuned_models_df = pd.DataFrame.from_dict(tuned_models_recall, orient='index')\n",
    "tuned_models_df = tuned_models_df.reset_index(drop=True)\n",
    "\n",
    "# Sort the models by recall score in descending order\n",
    "tuned_models_df = tuned_models_df.sort_values(by='Recall', ascending=False)\n",
    "\n",
    "print(tuned_models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided results, it appears that the Naive Bayes model has the highest recall score of approximately 0.50135, which is slightly higher than the other models. However, it's important to note that the difference in recall scores among these models is quite small, and other factors like training time should also be considered.\n",
    "\n",
    "Therefore, based solely on the recall score, Naive Bayes appears to be the best-performing model. However, you may also want to consider the trade-offs between model performance and training time. If the training time is a critical factor, you might choose a model like Logistic Regression, which has a reasonable recall score and a much lower training time.\n",
    "\n",
    "In summary, Naive Bayes is the best model in terms of recall score, but the choice of the best model may depend on other factors like training time and the specific requirements of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Retrain Naive Bayes  with Hyper parameter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV 2/3] END ..priors=None, var_smoothing=1e-09;, score=0.419 total time=   0.1s\n",
      "[CV 1/3] END ..priors=None, var_smoothing=1e-07;, score=0.471 total time=   0.1s\n",
      "[CV 3/3] END ..priors=None, var_smoothing=1e-09;, score=0.600 total time=   0.1s\n",
      "[CV 1/3] END ..priors=None, var_smoothing=1e-05;, score=0.471 total time=   0.1s\n",
      "[CV 1/3] END ..priors=None, var_smoothing=1e-06;, score=0.471 total time=   0.1s\n",
      "[CV 3/3] END ..priors=None, var_smoothing=1e-07;, score=0.600 total time=   0.1s\n",
      "[CV 2/3] END ..priors=None, var_smoothing=1e-05;, score=0.419 total time=   0.1s\n",
      "[CV 2/3] END ..priors=None, var_smoothing=1e-08;, score=0.419 total time=   0.1s\n",
      "[CV 1/3] END ..priors=None, var_smoothing=1e-09;, score=0.471 total time=   0.1s\n",
      "[CV 3/3] END ..priors=None, var_smoothing=1e-05;, score=0.600 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.2, 0.8], var_smoothing=1e-09;, score=1.000 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.2, 0.8], var_smoothing=1e-09;, score=1.000 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.2, 0.8], var_smoothing=1e-09;, score=1.000 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.2, 0.8], var_smoothing=1e-08;, score=1.000 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.2, 0.8], var_smoothing=1e-07;, score=1.000 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.2, 0.8], var_smoothing=1e-08;, score=1.000 total time=   0.1s\n",
      "[CV 2/3] END ..priors=None, var_smoothing=1e-06;, score=0.419 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.2, 0.8], var_smoothing=1e-08;, score=1.000 total time=   0.2s\n",
      "[CV 3/3] END priors=[0.2, 0.8], var_smoothing=1e-07;, score=1.000 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.2, 0.8], var_smoothing=1e-05;, score=1.000 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.2, 0.8], var_smoothing=1e-06;, score=1.000 total time=   0.1s[CV 3/3] END priors=[0.2, 0.8], var_smoothing=1e-06;, score=1.000 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.2, 0.8], var_smoothing=1e-06;, score=1.000 total time=   0.1s\n",
      "\n",
      "[CV 2/3] END ..priors=None, var_smoothing=1e-07;, score=0.419 total time=   0.2s\n",
      "[CV 2/3] END priors=[0.2, 0.8], var_smoothing=1e-07;, score=1.000 total time=   0.2s\n",
      "[CV 2/3] END priors=[0.2, 0.8], var_smoothing=1e-05;, score=1.000 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.5, 0.5], var_smoothing=1e-09;, score=0.523 total time=   0.1s\n",
      "[CV 3/3] END ..priors=None, var_smoothing=1e-08;, score=0.600 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.2, 0.8], var_smoothing=1e-05;, score=1.000 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.5, 0.5], var_smoothing=1e-09;, score=0.600 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.5, 0.5], var_smoothing=1e-08;, score=0.523 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.5, 0.5], var_smoothing=1e-09;, score=0.442 total time=   0.1s\n",
      "[CV 1/3] END ..priors=None, var_smoothing=1e-08;, score=0.471 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.5, 0.5], var_smoothing=1e-07;, score=0.442 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.5, 0.5], var_smoothing=1e-08;, score=0.442 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.5, 0.5], var_smoothing=1e-08;, score=0.600 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.5, 0.5], var_smoothing=1e-07;, score=0.523 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.5, 0.5], var_smoothing=1e-07;, score=0.600 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.5, 0.5], var_smoothing=1e-06;, score=0.442 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.5, 0.5], var_smoothing=1e-06;, score=0.523 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.5, 0.5], var_smoothing=1e-06;, score=0.600 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.5, 0.5], var_smoothing=1e-05;, score=0.523 total time=   0.1s\n",
      "[CV 3/3] END ..priors=None, var_smoothing=1e-06;, score=0.600 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.5, 0.5], var_smoothing=1e-05;, score=0.442 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.5, 0.5], var_smoothing=1e-05;, score=0.600 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.8, 0.2], var_smoothing=1e-09;, score=0.000 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.8, 0.2], var_smoothing=1e-09;, score=0.000 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.8, 0.2], var_smoothing=1e-09;, score=0.000 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.8, 0.2], var_smoothing=1e-08;, score=0.000 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.8, 0.2], var_smoothing=1e-08;, score=0.000 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.8, 0.2], var_smoothing=1e-08;, score=0.000 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.8, 0.2], var_smoothing=1e-07;, score=0.000 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.8, 0.2], var_smoothing=1e-07;, score=0.000 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.8, 0.2], var_smoothing=1e-07;, score=0.000 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.8, 0.2], var_smoothing=1e-06;, score=0.000 total time=   0.1s\n",
      "[CV 3/3] END priors=[0.8, 0.2], var_smoothing=1e-06;, score=0.000 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.8, 0.2], var_smoothing=1e-06;, score=0.000 total time=   0.1s\n",
      "[CV 1/3] END priors=[0.8, 0.2], var_smoothing=1e-05;, score=0.000 total time=   0.1s\n",
      "[CV 2/3] END priors=[0.8, 0.2], var_smoothing=1e-05;, score=0.000 total time=   0.0s\n",
      "[CV 3/3] END priors=[0.8, 0.2], var_smoothing=1e-05;, score=0.000 total time=   0.0s\n",
      "Best Parameters for Naive Bayes (Fine-Tuned): {'priors': [0.2, 0.8], 'var_smoothing': 1e-09}\n",
      "Train Recall Score for Naive Bayes (Fine-Tuned): 0.4977875\n",
      "Test Recall Score for Naive Bayes (Fine-Tuned): 0.4978\n",
      "Train Accuracy Score for Naive Bayes (Fine-Tuned): 0.4977875\n",
      "Test Accuracy Score for Naive Bayes (Fine-Tuned): 0.4978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "# Define the Naive Bayes model\n",
    "naive_bayes_model = GaussianNB()\n",
    "\n",
    "# Define hyperparameter grid for Naive Bayes fine-tuning\n",
    "param_grid = {\n",
    "    'priors': [None, [0.2, 0.8], [0.5, 0.5], [0.8, 0.2]],\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5],\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the Naive Bayes model and hyperparameter grid\n",
    "grid_search_nb = GridSearchCV(naive_bayes_model, param_grid, scoring='recall', cv=3, n_jobs=-1, verbose=3)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search_nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Get the best Naive Bayes model with fine-tuned hyperparameters\n",
    "best_naive_bayes_model = grid_search_nb.best_estimator_\n",
    "\n",
    "# Retrain the best model on the full training data\n",
    "best_naive_bayes_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Make predictions on the train and test data\n",
    "y_train_pred_nb = best_naive_bayes_model.predict(X_train_transformed)\n",
    "y_test_pred_nb = best_naive_bayes_model.predict(X_test_transformed)\n",
    "\n",
    "# Calculate recall scores for train and test data predictions\n",
    "recall_train_nb = recall_score(y_train, y_train_pred_nb, average='micro')\n",
    "recall_test_nb = recall_score(y_test, y_test_pred_nb, average='micro')\n",
    "\n",
    "# Calculate accuracy scores for train and test data predictions\n",
    "accuracy_train_nb = accuracy_score(y_train, y_train_pred_nb)\n",
    "accuracy_test_nb = accuracy_score(y_test, y_test_pred_nb)\n",
    "\n",
    "# Get the best hyperparameters for Naive Bayes\n",
    "best_params_nb = grid_search_nb.best_params_\n",
    "\n",
    "# Print the best hyperparameters and train/test recall and accuracy scores\n",
    "print(\"Best Parameters for Naive Bayes (Fine-Tuned):\", best_params_nb)\n",
    "print(\"Train Recall Score for Naive Bayes (Fine-Tuned):\", recall_train_nb)\n",
    "print(\"Test Recall Score for Naive Bayes (Fine-Tuned):\", recall_test_nb)\n",
    "print(\"Train Accuracy Score for Naive Bayes (Fine-Tuned):\", accuracy_train_nb)\n",
    "print(\"Test Accuracy Score for Naive Bayes (Fine-Tuned):\", accuracy_test_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Here are the insights based on the fine-tuned Naive Bayes model:`\n",
    "\n",
    "1. **Best Hyperparameters**: The best hyperparameters for the Naive Bayes model after fine-tuning are:\n",
    "   - Prior Probabilities: [0.2, 0.8]\n",
    "   - Smoothing Variance: 1e-09\n",
    "\n",
    "2. **Recall Scores**:\n",
    "   - Train Recall Score: The recall score on the training data is approximately 0.4978, indicating that the model correctly identifies around 49.78% of the positive instances in the training set.\n",
    "   - Test Recall Score: The recall score on the test data is also approximately 0.4978, suggesting that the model performs similarly on unseen data, correctly identifying around 49.78% of the positive instances in the test set.\n",
    "\n",
    "3. **Accuracy Scores**:\n",
    "   - Train Accuracy Score: The accuracy score on the training data is approximately 0.4978, indicating that the model correctly predicts around 49.78% of all instances in the training set.\n",
    "   - Test Accuracy Score: The accuracy score on the test data is also approximately 0.4978, suggesting that the model's predictions are accurate for around 49.78% of all instances in the test set.\n",
    "\n",
    "4. **Model Performance**: The fine-tuned Naive Bayes model achieves fairly balanced recall and accuracy scores on both the training and test datasets. However, the scores are approximately 0.4978, which may not be very high. Depending on the specific problem and requirements, further model exploration or alternative algorithms may be considered to improve predictive performance.\n",
    "\n",
    "5. **Hyperparameters**: The chosen hyperparameters, such as the prior probabilities and smoothing variance, reflect the model's sensitivity to class priors and the amount of smoothing applied to feature variances. These values have been determined as the best combination for this particular model and dataset through the hyperparameter tuning process.\n",
    "\n",
    "6. **Further Optimization**: To potentially improve model performance, you could explore different machine learning algorithms, feature engineering techniques, or collect more data if possible. Additionally, consider the business or problem context to determine if the current performance meets the desired objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Not Churn       0.00      0.00      0.00     40177\n",
      "       Churn       0.50      1.00      0.66     39823\n",
      "\n",
      "    accuracy                           0.50     80000\n",
      "   macro avg       0.25      0.50      0.33     80000\n",
      "weighted avg       0.25      0.50      0.33     80000\n",
      "\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Not Churn       0.00      0.00      0.00     10044\n",
      "       Churn       0.50      1.00      0.66      9956\n",
      "\n",
      "    accuracy                           0.50     20000\n",
      "   macro avg       0.25      0.50      0.33     20000\n",
      "weighted avg       0.25      0.50      0.33     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_train contains the true labels for the training data\n",
    "# and y_test contains the true labels for the test data\n",
    "# Replace y_train_pred_nb and y_test_pred_nb with your predicted labels\n",
    "confusion_train = confusion_matrix(y_train, y_train_pred_nb)\n",
    "confusion_test = confusion_matrix(y_test, y_test_pred_nb)\n",
    "\n",
    "# Get classification report for train and test data\n",
    "report_train = classification_report(y_train, y_train_pred_nb, target_names=['Not Churn', 'Churn'])\n",
    "report_test = classification_report(y_test, y_test_pred_nb, target_names=['Not Churn', 'Churn'])\n",
    "\n",
    "# Create subplots for train and test confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Create heatmaps for train and test confusion matrices with larger font size\n",
    "sns.heatmap(confusion_train, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=['Not Churn', 'Churn'], yticklabels=['Not Churn', 'Churn'], ax=axes[0], annot_kws={\"size\": 16})\n",
    "sns.heatmap(confusion_test, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=['Not Churn', 'Churn'], yticklabels=['Not Churn', 'Churn'], ax=axes[1], annot_kws={\"size\": 16})\n",
    "\n",
    "# Set titles and labels for train and test confusion matrices\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_title('Train Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "axes[1].set_title('Test Confusion Matrix')\n",
    "\n",
    "# Display the classification report\n",
    "print(\"Train Classification Report:\\n\", report_train)\n",
    "print(\"\\nTest Classification Report:\\n\", report_test)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The classification reports show the precision, recall, F1-score, and support for both the training and test datasets. Here's a breakdown of the key metrics:**\n",
    "\n",
    "**For the Training Dataset:**\n",
    "- Precision for Churn: 0.50\n",
    "- Recall for Churn: 1.00\n",
    "- F1-score for Churn: 0.66\n",
    "\n",
    "**For the Test Dataset:**\n",
    "- Precision for Churn: 0.50\n",
    "- Recall for Churn: 1.00\n",
    "- F1-score for Churn: 0.66\n",
    "\n",
    "It appears that the model is achieving a perfect recall for the \"Churn\" class, which means it correctly identifies all instances of churn in both the training and test datasets. However, the precision is relatively low, indicating that there might be a high number of false positives.\n",
    "\n",
    "Overall, while the recall for the \"Churn\" class is high, the model's performance in terms of precision and the F1-score is suboptimal. This suggests that the model may be overly biased towards predicting the \"Churn\" class, resulting in a high number of false positives for \"Not Churn.\" Further model tuning and evaluation may be necessary to improve overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as src/churn/data/customer_churn_large_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Converting the xlsx file to csv cause it is lighter and loads quickly\n",
    "file_path = \"src/churn/data/customer_churn_large_dataset.csv\"\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "churn.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame saved as {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Location</th>\n",
       "      <th>Subscription_Length_Months</th>\n",
       "      <th>Monthly_Bill</th>\n",
       "      <th>Total_Usage_GB</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>Male</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>17</td>\n",
       "      <td>73.36</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>Female</td>\n",
       "      <td>New York</td>\n",
       "      <td>1</td>\n",
       "      <td>48.76</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>5</td>\n",
       "      <td>85.47</td>\n",
       "      <td>460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>Female</td>\n",
       "      <td>Miami</td>\n",
       "      <td>3</td>\n",
       "      <td>97.94</td>\n",
       "      <td>297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>Female</td>\n",
       "      <td>Miami</td>\n",
       "      <td>19</td>\n",
       "      <td>58.14</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>33</td>\n",
       "      <td>Male</td>\n",
       "      <td>Houston</td>\n",
       "      <td>23</td>\n",
       "      <td>55.13</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>62</td>\n",
       "      <td>Female</td>\n",
       "      <td>New York</td>\n",
       "      <td>19</td>\n",
       "      <td>61.65</td>\n",
       "      <td>351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>64</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>17</td>\n",
       "      <td>96.11</td>\n",
       "      <td>251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>51</td>\n",
       "      <td>Female</td>\n",
       "      <td>New York</td>\n",
       "      <td>20</td>\n",
       "      <td>49.25</td>\n",
       "      <td>434</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>27</td>\n",
       "      <td>Female</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>19</td>\n",
       "      <td>76.57</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Gender     Location  Subscription_Length_Months  Monthly_Bill  \\\n",
       "0       63    Male  Los Angeles                          17         73.36   \n",
       "1       62  Female     New York                           1         48.76   \n",
       "2       24  Female  Los Angeles                           5         85.47   \n",
       "3       36  Female        Miami                           3         97.94   \n",
       "4       46  Female        Miami                          19         58.14   \n",
       "...    ...     ...          ...                         ...           ...   \n",
       "99995   33    Male      Houston                          23         55.13   \n",
       "99996   62  Female     New York                          19         61.65   \n",
       "99997   64    Male      Chicago                          17         96.11   \n",
       "99998   51  Female     New York                          20         49.25   \n",
       "99999   27  Female  Los Angeles                          19         76.57   \n",
       "\n",
       "       Total_Usage_GB  Churn  \n",
       "0                 236      0  \n",
       "1                 172      0  \n",
       "2                 460      0  \n",
       "3                 297      1  \n",
       "4                 266      0  \n",
       "...               ...    ...  \n",
       "99995             226      1  \n",
       "99996             351      0  \n",
       "99997             251      1  \n",
       "99998             434      1  \n",
       "99999             173      1  \n",
       "\n",
       "[100000 rows x 7 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Los Angeles', 'New York', 'Miami', 'Chicago', 'Houston'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn['Location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Male', 'Female'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn['Gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 7 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   Age                         100000 non-null  int64  \n",
      " 1   Gender                      100000 non-null  object \n",
      " 2   Location                    100000 non-null  object \n",
      " 3   Subscription_Length_Months  100000 non-null  int64  \n",
      " 4   Monthly_Bill                100000 non-null  float64\n",
      " 5   Total_Usage_GB              100000 non-null  int64  \n",
      " 6   Churn                       100000 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(2)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "churn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
